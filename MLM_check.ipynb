{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "GxhHSrx4mm6J",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "import zipfile, os\n",
    "zip_ref = zipfile.ZipFile('mongolian-bert-master.zip', 'r')\n",
    "zip_ref.extractall('./')\n",
    "os.chdir('./mongolian-bert-master/')\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "Ms2rDOo7nMXz",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "from google.colab import auth\n",
    "auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "OUxUJ_6Am0Va",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 819.0
    },
    "outputId": "a3e83817-ee81-4e87-c0fe-f18a444b882a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 1)) (3.2.5)\n",
      "Collecting sentencepiece (from -r requirements.txt (line 2))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7e/8a/0e4a10bc00a0263db8d45d0062c83892598eb58e8091f439c63926e9b107/sentencepiece-0.1.81-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.0MB 17.6MB/s \n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 3)) (2.18.4)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 4)) (4.28.1)\n",
      "Collecting patool (from -r requirements.txt (line 5))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/43/94/52243ddff508780dd2d8110964320ab4851134a55ab102285b46e740f76a/patool-1.12-py2.py3-none-any.whl (77kB)\n",
      "\u001b[K    100% |████████████████████████████████| 81kB 26.8MB/s \n",
      "\u001b[?25hCollecting EbookLib (from -r requirements.txt (line 6))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/00/38/7d6ab2e569a9165249619d73b7bc6be0e713a899a3bc2513814b6598a84c/EbookLib-0.17.1.tar.gz (111kB)\n",
      "\u001b[K    100% |████████████████████████████████| 112kB 34.1MB/s \n",
      "\u001b[?25hRequirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 7)) (4.6.3)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk->-r requirements.txt (line 1)) (1.11.0)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->-r requirements.txt (line 3)) (1.22)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->-r requirements.txt (line 3)) (2.6)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->-r requirements.txt (line 3)) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->-r requirements.txt (line 3)) (2019.3.9)\n",
      "Requirement already satisfied: lxml in /usr/local/lib/python3.6/dist-packages (from EbookLib->-r requirements.txt (line 6)) (4.2.6)\n",
      "Building wheels for collected packages: EbookLib\n",
      "  Building wheel for EbookLib (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/84/11/01/951369cbbf8f96878786a1f4da68bd7ac19a5d945b38e03d54\n",
      "Successfully built EbookLib\n",
      "Installing collected packages: sentencepiece, patool, EbookLib\n",
      "Successfully installed EbookLib-0.17.1 patool-1.12 sentencepiece-0.1.81\n",
      "Collecting pytorch-pretrained-bert\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5d/3c/d5fa084dd3a82ffc645aba78c417e6072ff48552e3301b1fa3bd711e03d4/pytorch_pretrained_bert-0.6.1-py3-none-any.whl (114kB)\n",
      "\u001b[K    100% |████████████████████████████████| 122kB 3.8MB/s \n",
      "\u001b[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.9.115)\n",
      "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.0.1.post2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2.18.4)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (4.28.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.14.6)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2018.1.10)\n",
      "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.2.0)\n",
      "Requirement already satisfied: botocore<1.13.0,>=1.12.115 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (1.12.115)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.9.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2019.3.9)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2.6)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (1.22)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.115->boto3->pytorch-pretrained-bert) (2.5.3)\n",
      "Requirement already satisfied: docutils>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.115->boto3->pytorch-pretrained-bert) (0.14)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\"->botocore<1.13.0,>=1.12.115->boto3->pytorch-pretrained-bert) (1.11.0)\n",
      "Installing collected packages: pytorch-pretrained-bert\n",
      "Successfully installed pytorch-pretrained-bert-0.6.1\n"
     ]
    }
   ],
   "source": [
    "!pip3 install -r requirements.txt\n",
    "!pip3 install pytorch-pretrained-bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "vR7kydxjnIRJ",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from tokenization_sentencepiece import FullTokenizer\n",
    "from pytorch_pretrained_bert import BertModel, BertForMaskedLM, BertForNextSentencePrediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "XAzb4iTYoTck",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "oqBDRmG5ogFZ",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34.0
    },
    "outputId": "56485213-4977-4943-b812-d152fc6c6d83"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded a trained SentencePiece model.\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained model tokenizer \n",
    "tokenizer = FullTokenizer(model_file='model-32k/mn_cased.model', vocab_file='model-32k/mn_cased.vocab', do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "L6WqRs71ordS",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "text = \"Монгол бахархлын өдөр буюу Их эзэн Чингис хааны мэндэлсэн өдөр өчигдөр тохиов Эрдэмтэд Чингис хааны мэндэлсэн өдрийг билгийн тооллын гуравдугаар жарны усан морин жил буюу 1162 оны өвлийн тэргүүн сарын шинийн 1-ний өдөр хэмээн тогтоосон байдаг\"\n",
    "tokenized_text = tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "fGU_V7izqHsx",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54.0
    },
    "outputId": "05b43f3d-9844-4ada-979f-82adfa9bac5c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁Монгол', '▁бахархлын', '▁өдөр', '▁буюу', '▁Их', '▁эзэн', '[MASK]', '▁хааны', '▁мэндэлсэн', '▁өдөр', '▁өчигдөр', '▁тохио', 'в', '▁Эрдэмтэд', '▁Чингис', '▁хааны', '▁мэндэлсэн', '▁өдрийг', '▁билгийн', '▁тооллын', '▁гуравдугаар', '▁жарны', '▁усан', '▁морин', '▁жил', '▁буюу', '▁11', '62', '▁оны', '▁өвлийн', '▁тэргүүн', '▁сарын', '▁шинийн', '▁1-', 'ний', '▁өдөр', '▁хэмээн', '▁тогтоосон', '▁байдаг']\n"
     ]
    }
   ],
   "source": [
    "masked_index = 6\n",
    "tokenized_text[masked_index] = '[MASK]'\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "BmxyYJ8xsFZY",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "# Convert token to vocabulary indices\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "# Define sentence A and B indices associated to 1st and 2nd sentences (see paper)\n",
    "segments_ids = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "bPWM5YO6uQM7",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34.0
    },
    "outputId": "32d178a2-7589-455c-e9af-0eefca588c74"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(segments_ids) == len(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "TqeUqR0DujA9",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "# Convert inputs to PyTorch tensors\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "segments_tensors = torch.tensor([segments_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "FG1YiKGQw0BU",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "mkdir model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "_t6H9tvjvznd",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170.0
    },
    "outputId": "d9d39e47-06fc-40ff-fd30-a001ea44c257"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://mongolian-bert/model-32k/model/model.ckpt-952000.meta...\n",
      "/ [1 files][  4.7 MiB/  4.7 MiB]                                                \n",
      "Operation completed over 1 objects/4.7 MiB.                                      \n",
      "Copying gs://mongolian-bert/model-32k/model/model.ckpt-952000.index...\n",
      "/ [1 files][  9.1 KiB/  9.1 KiB]                                                \n",
      "Operation completed over 1 objects/9.1 KiB.                                      \n",
      "Copying gs://mongolian-bert/model-32k/model/model.ckpt-952000.data-00000-of-00001...\n",
      "- [1 files][  1.2 GiB/  1.2 GiB]   61.1 MiB/s                                   \n",
      "Operation completed over 1 objects/1.2 GiB.                                      \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp gs://mongolian-bert/model-32k/model/model.ckpt-952000.meta model/\n",
    "!gsutil cp gs://mongolian-bert/model-32k/model/model.ckpt-952000.index model/\n",
    "!gsutil cp gs://mongolian-bert/model-32k/model/model.ckpt-952000.data-00000-of-00001 model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "HlWFcOkjxpEr",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "!cp model-32k/bert_config.json model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "N7eedmAwyifY",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 21352.0
    },
    "outputId": "4a8c3ead-5204-4a5b-b66e-f53c8a1948cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n",
      "Converting TensorFlow checkpoint from /content/mongolian-bert-master/model/model.ckpt-952000 with config at /content/mongolian-bert-master/model/bert_config.json\n",
      "Loading TF weight bert/embeddings/LayerNorm/beta with shape [768]\n",
      "Loading TF weight bert/embeddings/LayerNorm/beta/adam_m with shape [768]\n",
      "Loading TF weight bert/embeddings/LayerNorm/beta/adam_v with shape [768]\n",
      "Loading TF weight bert/embeddings/LayerNorm/gamma with shape [768]\n",
      "Loading TF weight bert/embeddings/LayerNorm/gamma/adam_m with shape [768]\n",
      "Loading TF weight bert/embeddings/LayerNorm/gamma/adam_v with shape [768]\n",
      "Loading TF weight bert/embeddings/position_embeddings with shape [512, 768]\n",
      "Loading TF weight bert/embeddings/position_embeddings/adam_m with shape [512, 768]\n",
      "Loading TF weight bert/embeddings/position_embeddings/adam_v with shape [512, 768]\n",
      "Loading TF weight bert/embeddings/token_type_embeddings with shape [2, 768]\n",
      "Loading TF weight bert/embeddings/token_type_embeddings/adam_m with shape [2, 768]\n",
      "Loading TF weight bert/embeddings/token_type_embeddings/adam_v with shape [2, 768]\n",
      "Loading TF weight bert/embeddings/word_embeddings with shape [32000, 768]\n",
      "Loading TF weight bert/embeddings/word_embeddings/adam_m with shape [32000, 768]\n",
      "Loading TF weight bert/embeddings/word_embeddings/adam_v with shape [32000, 768]\n",
      "Loading TF weight bert/encoder/layer_0/attention/output/LayerNorm/beta with shape [768]\n",
      "Loading TF weight bert/encoder/layer_0/attention/output/LayerNorm/beta/adam_m with shape [768]\n",
      "Loading TF weight bert/encoder/layer_0/attention/output/LayerNorm/beta/adam_v with shape [768]\n",
      "Loading TF weight bert/encoder/layer_0/attention/output/LayerNorm/gamma with shape [768]\n",
      "Loading TF weight bert/encoder/layer_0/attention/output/LayerNorm/gamma/adam_m with shape [768]\n",
      "Loading TF weight bert/encoder/layer_0/attention/output/LayerNorm/gamma/adam_v with shape [768]\n",
      "Loading TF weight bert/encoder/layer_0/attention/output/dense/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_0/attention/output/dense/bias/adam_m with shape [768]\n",
      "Loading TF weight bert/encoder/layer_0/attention/output/dense/bias/adam_v with shape [768]\n",
      "Loading TF weight bert/encoder/layer_0/attention/output/dense/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_0/attention/output/dense/kernel/adam_m with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_0/attention/output/dense/kernel/adam_v with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_0/attention/self/key/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_0/attention/self/key/bias/adam_m with shape [768]\n",
      "Loading TF weight bert/encoder/layer_0/attention/self/key/bias/adam_v with shape [768]\n",
      "Loading TF weight bert/encoder/layer_0/attention/self/key/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_0/attention/self/key/kernel/adam_m with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_0/attention/self/key/kernel/adam_v with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_0/attention/self/query/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_0/attention/self/query/bias/adam_m with shape [768]\n",
      "Loading TF weight bert/encoder/layer_0/attention/self/query/bias/adam_v with shape [768]\n",
      "Loading TF weight bert/encoder/layer_0/attention/self/query/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_0/attention/self/query/kernel/adam_m with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_0/attention/self/query/kernel/adam_v with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_0/attention/self/value/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_0/attention/self/value/bias/adam_m with shape [768]\n",
      "Loading TF weight bert/encoder/layer_0/attention/self/value/bias/adam_v with shape [768]\n",
      "Loading TF weight bert/encoder/layer_0/attention/self/value/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_0/attention/self/value/kernel/adam_m with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_0/attention/self/value/kernel/adam_v with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_0/intermediate/dense/bias with shape [3072]\n",
      "Loading TF weight bert/encoder/layer_0/intermediate/dense/bias/adam_m with shape [3072]\n",
      "Loading TF weight bert/encoder/layer_0/intermediate/dense/bias/adam_v with shape [3072]\n",
      "Loading TF weight bert/encoder/layer_0/intermediate/dense/kernel with shape [768, 3072]\n",
      "Loading TF weight bert/encoder/layer_0/intermediate/dense/kernel/adam_m with shape [768, 3072]\n",
      "Loading TF weight bert/encoder/layer_0/intermediate/dense/kernel/adam_v with shape [768, 3072]\n",
      "Loading TF weight bert/encoder/layer_0/output/LayerNorm/beta with shape [768]\n",
      "Loading TF weight bert/encoder/layer_0/output/LayerNorm/beta/adam_m with shape [768]\n",
      "Loading TF weight bert/encoder/layer_0/output/LayerNorm/beta/adam_v with shape [768]\n",
      "Loading TF weight bert/encoder/layer_0/output/LayerNorm/gamma with shape [768]\n",
      "Loading TF weight bert/encoder/layer_0/output/LayerNorm/gamma/adam_m with shape [768]\n",
      "Loading TF weight bert/encoder/layer_0/output/LayerNorm/gamma/adam_v with shape [768]\n",
      "Loading TF weight bert/encoder/layer_0/output/dense/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_0/output/dense/bias/adam_m with shape [768]\n",
      "Loading TF weight bert/encoder/layer_0/output/dense/bias/adam_v with shape [768]\n",
      "Loading TF weight bert/encoder/layer_0/output/dense/kernel with shape [3072, 768]\n",
      "Loading TF weight bert/encoder/layer_0/output/dense/kernel/adam_m with shape [3072, 768]\n",
      "Loading TF weight bert/encoder/layer_0/output/dense/kernel/adam_v with shape [3072, 768]\n",
      "Loading TF weight bert/encoder/layer_1/attention/output/LayerNorm/beta with shape [768]\n",
      "Loading TF weight bert/encoder/layer_1/attention/output/LayerNorm/beta/adam_m with shape [768]\n",
      "Loading TF weight bert/encoder/layer_1/attention/output/LayerNorm/beta/adam_v with shape [768]\n",
      "Loading TF weight bert/encoder/layer_1/attention/output/LayerNorm/gamma with shape [768]\n",
      "Loading TF weight bert/encoder/layer_1/attention/output/LayerNorm/gamma/adam_m with shape [768]\n",
      "Loading TF weight bert/encoder/layer_1/attention/output/LayerNorm/gamma/adam_v with shape [768]\n",
      "Loading TF weight bert/encoder/layer_1/attention/output/dense/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_1/attention/output/dense/bias/adam_m with shape [768]\n",
      "Loading TF weight bert/encoder/layer_1/attention/output/dense/bias/adam_v with shape [768]\n",
      "Loading TF weight bert/encoder/layer_1/attention/output/dense/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_1/attention/output/dense/kernel/adam_m with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_1/attention/output/dense/kernel/adam_v with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_1/attention/self/key/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_1/attention/self/key/bias/adam_m with shape [768]\n",
      "Loading TF weight bert/encoder/layer_1/attention/self/key/bias/adam_v with shape [768]\n",
      "Loading TF weight bert/encoder/layer_1/attention/self/key/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_1/attention/self/key/kernel/adam_m with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_1/attention/self/key/kernel/adam_v with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_1/attention/self/query/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_1/attention/self/query/bias/adam_m with shape [768]\n",
      "Loading TF weight bert/encoder/layer_1/attention/self/query/bias/adam_v with shape [768]\n",
      "Loading TF weight bert/encoder/layer_1/attention/self/query/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_1/attention/self/query/kernel/adam_m with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_1/attention/self/query/kernel/adam_v with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_1/attention/self/value/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_1/attention/self/value/bias/adam_m with shape [768]\n",
      "Loading TF weight bert/encoder/layer_1/attention/self/value/bias/adam_v with shape [768]\n",
      "Loading TF weight bert/encoder/layer_1/attention/self/value/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_1/attention/self/value/kernel/adam_m with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_1/attention/self/value/kernel/adam_v with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_1/intermediate/dense/bias with shape [3072]\n",
      "Loading TF weight bert/encoder/layer_1/intermediate/dense/bias/adam_m with shape [3072]\n",
      "Loading TF weight bert/encoder/layer_1/intermediate/dense/bias/adam_v with shape [3072]\n",
      "Loading TF weight bert/encoder/layer_1/intermediate/dense/kernel with shape [768, 3072]\n",
      "Loading TF weight bert/encoder/layer_1/intermediate/dense/kernel/adam_m with shape [768, 3072]\n",
      "Loading TF weight bert/encoder/layer_1/intermediate/dense/kernel/adam_v with shape [768, 3072]\n",
      "Loading TF weight bert/encoder/layer_1/output/LayerNorm/beta with shape [768]\n",
      "Loading TF weight bert/encoder/layer_1/output/LayerNorm/beta/adam_m with shape [768]\n",
      "Loading TF weight bert/encoder/layer_1/output/LayerNorm/beta/adam_v with shape [768]\n",
      "Loading TF weight bert/encoder/layer_1/output/LayerNorm/gamma with shape [768]\n",
      "Loading TF weight bert/encoder/layer_1/output/LayerNorm/gamma/adam_m with shape [768]\n",
      "Loading TF weight bert/encoder/layer_1/output/LayerNorm/gamma/adam_v with shape [768]\n",
      "Loading TF weight bert/encoder/layer_1/output/dense/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_1/output/dense/bias/adam_m with shape [768]\n",
      "Loading TF weight bert/encoder/layer_1/output/dense/bias/adam_v with shape [768]\n",
      "Loading TF weight bert/encoder/layer_1/output/dense/kernel with shape [3072, 768]\n",
      "Loading TF weight bert/encoder/layer_1/output/dense/kernel/adam_m with shape [3072, 768]\n",
      "Loading TF weight bert/encoder/layer_1/output/dense/kernel/adam_v with shape [3072, 768]\n",
      "Loading TF weight bert/encoder/layer_10/attention/output/LayerNorm/beta with shape [768]\n",
      "Loading TF weight bert/encoder/layer_10/attention/output/LayerNorm/beta/adam_m with shape [768]\n",
      "Loading TF weight bert/encoder/layer_10/attention/output/LayerNorm/beta/adam_v with shape [768]\n",
      "Loading TF weight bert/encoder/layer_10/attention/output/LayerNorm/gamma with shape [768]\n",
      "Loading TF weight bert/encoder/layer_10/attention/output/LayerNorm/gamma/adam_m with shape [768]\n",
      "Loading TF weight bert/encoder/layer_10/attention/output/LayerNorm/gamma/adam_v with shape [768]\n",
      "Loading TF weight bert/encoder/layer_10/attention/output/dense/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_10/attention/output/dense/bias/adam_m with shape [768]\n",
      "Loading TF weight bert/encoder/layer_10/attention/output/dense/bias/adam_v with shape [768]\n",
      "Loading TF weight bert/encoder/layer_10/attention/output/dense/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_10/attention/output/dense/kernel/adam_m with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_10/attention/output/dense/kernel/adam_v with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_10/attention/self/key/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_10/attention/self/key/bias/adam_m with shape [768]\n",
      "Loading TF weight bert/encoder/layer_10/attention/self/key/bias/adam_v with shape [768]\n",
      "Loading TF weight bert/encoder/layer_10/attention/self/key/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_10/attention/self/key/kernel/adam_m with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_10/attention/self/key/kernel/adam_v with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_10/attention/self/query/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_10/attention/self/query/bias/adam_m with shape [768]\n",
      "Loading TF weight bert/encoder/layer_10/attention/self/query/bias/adam_v with shape [768]\n",
      "Loading TF weight bert/encoder/layer_10/attention/self/query/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_10/attention/self/query/kernel/adam_m with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_10/attention/self/query/kernel/adam_v with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_10/attention/self/value/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_10/attention/self/value/bias/adam_m with shape [768]\n",
      "Loading TF weight bert/encoder/layer_10/attention/self/value/bias/adam_v with shape [768]\n",
      "Loading TF weight bert/encoder/layer_10/attention/self/value/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_10/attention/self/value/kernel/adam_m with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_10/attention/self/value/kernel/adam_v with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_10/intermediate/dense/bias with shape [3072]\n",
      "Loading TF weight bert/encoder/layer_10/intermediate/dense/bias/adam_m with shape [3072]\n",
      "Loading TF weight bert/encoder/layer_10/intermediate/dense/bias/adam_v with shape [3072]\n",
      "Loading TF weight bert/encoder/layer_10/intermediate/dense/kernel with shape [768, 3072]\n",
      "Loading TF weight bert/encoder/layer_10/intermediate/dense/kernel/adam_m with shape [768, 3072]\n",
      "Loading TF weight bert/encoder/layer_10/intermediate/dense/kernel/adam_v with shape [768, 3072]\n",
      "Loading TF weight bert/encoder/layer_10/output/LayerNorm/beta with shape [768]\n",
      "Loading TF weight bert/encoder/layer_10/output/LayerNorm/beta/adam_m with shape [768]\n",
      "Loading TF weight bert/encoder/layer_10/output/LayerNorm/beta/adam_v with shape [768]\n",
      "Loading TF weight bert/encoder/layer_10/output/LayerNorm/gamma with shape [768]\n",
      "Loading TF weight bert/encoder/layer_10/output/LayerNorm/gamma/adam_m with shape [768]\n",
      "Loading TF weight bert/encoder/layer_10/output/LayerNorm/gamma/adam_v with shape [768]\n",
      "Loading TF weight bert/encoder/layer_10/output/dense/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_10/output/dense/bias/adam_m with shape [768]\n",
      "Loading TF weight bert/encoder/layer_10/output/dense/bias/adam_v with shape [768]\n",
      "Loading TF weight bert/encoder/layer_10/output/dense/kernel with shape [3072, 768]\n",
      "Loading TF weight bert/encoder/layer_10/output/dense/kernel/adam_m with shape [3072, 768]\n",
      "Loading TF weight bert/encoder/layer_10/output/dense/kernel/adam_v with shape [3072, 768]\n",
      "Loading TF weight bert/encoder/layer_11/attention/output/LayerNorm/beta with shape [768]\n",
      "Loading TF weight bert/encoder/layer_11/attention/output/LayerNorm/beta/adam_m with shape [768]\n",
      "Loading TF weight bert/encoder/layer_11/attention/output/LayerNorm/beta/adam_v with shape [768]\n",
      "Loading TF weight bert/encoder/layer_11/attention/output/LayerNorm/gamma with shape [768]\n",
      "Loading TF weight bert/encoder/layer_11/attention/output/LayerNorm/gamma/adam_m with shape [768]\n",
      "Loading TF weight bert/encoder/layer_11/attention/output/LayerNorm/gamma/adam_v with shape [768]\n",
      "Loading TF weight bert/encoder/layer_11/attention/output/dense/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_11/attention/output/dense/bias/adam_m with shape [768]\n",
      "Loading TF weight bert/encoder/layer_11/attention/output/dense/bias/adam_v with shape [768]\n",
      "Loading TF weight bert/encoder/layer_11/attention/output/dense/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_11/attention/output/dense/kernel/adam_m with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_11/attention/output/dense/kernel/adam_v with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_11/attention/self/key/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_11/attention/self/key/bias/adam_m with shape [768]\n",
      "Loading TF weight bert/encoder/layer_11/attention/self/key/bias/adam_v with shape [768]\n",
      "Loading TF weight bert/encoder/layer_11/attention/self/key/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_11/attention/self/key/kernel/adam_m with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_11/attention/self/key/kernel/adam_v with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_11/attention/self/query/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_11/attention/self/query/bias/adam_m with shape [768]\n",
      "Loading TF weight bert/encoder/layer_11/attention/self/query/bias/adam_v with shape [768]\n",
      "Loading TF weight bert/encoder/layer_11/attention/self/query/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_11/attention/self/query/kernel/adam_m with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_11/attention/self/query/kernel/adam_v with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_11/attention/self/value/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_11/attention/self/value/bias/adam_m with shape [768]\n",
      "Loading TF weight bert/encoder/layer_11/attention/self/value/bias/adam_v with shape [768]\n",
      "Loading TF weight bert/encoder/layer_11/attention/self/value/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_11/attention/self/value/kernel/adam_m with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_11/attention/self/value/kernel/adam_v with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_11/intermediate/dense/bias with shape [3072]\n",
      "Loading TF weight bert/encoder/layer_11/intermediate/dense/bias/adam_m with shape [3072]\n",
      "Loading TF weight bert/encoder/layer_11/intermediate/dense/bias/adam_v with shape [3072]\n",
      "Loading TF weight bert/encoder/layer_11/intermediate/dense/kernel with shape [768, 3072]\n",
      "Loading TF weight bert/encoder/layer_11/intermediate/dense/kernel/adam_m with shape [768, 3072]\n",
      "Loading TF weight bert/encoder/layer_11/intermediate/dense/kernel/adam_v with shape [768, 3072]\n",
      "Loading TF weight bert/encoder/layer_11/output/LayerNorm/beta with shape [768]\n",
      "Loading TF weight bert/encoder/layer_11/output/LayerNorm/beta/adam_m with shape [768]\n",
      "Loading TF weight bert/encoder/layer_11/output/LayerNorm/beta/adam_v with shape [768]\n",
      "Loading TF weight bert/encoder/layer_11/output/LayerNorm/gamma with shape [768]\n",
      "Loading TF weight bert/encoder/layer_11/output/LayerNorm/gamma/adam_m with shape [768]\n",
      "Loading TF weight bert/encoder/layer_11/output/LayerNorm/gamma/adam_v with shape [768]\n",
      "Loading TF weight bert/encoder/layer_11/output/dense/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_11/output/dense/bias/adam_m with shape [768]\n",
      "Loading TF weight bert/encoder/layer_11/output/dense/bias/adam_v with shape [768]\n",
      "Loading TF weight bert/encoder/layer_11/output/dense/kernel with shape [3072, 768]\n",
      "Loading TF weight bert/encoder/layer_11/output/dense/kernel/adam_m with shape [3072, 768]\n",
      "Loading TF weight bert/encoder/layer_11/output/dense/kernel/adam_v with shape [3072, 768]\n",
      "Loading TF weight bert/encoder/layer_2/attention/output/LayerNorm/beta with shape [768]\n",
      "Loading TF weight bert/encoder/layer_2/attention/output/LayerNorm/beta/adam_m with shape [768]\n",
      "Loading TF weight bert/encoder/layer_2/attention/output/LayerNorm/beta/adam_v with shape [768]\n",
      "Loading TF weight bert/encoder/layer_2/attention/output/LayerNorm/gamma with shape [768]\n",
      "Loading TF weight bert/encoder/layer_2/attention/output/LayerNorm/gamma/adam_m with shape [768]\n",
      "Loading TF weight bert/encoder/layer_2/attention/output/LayerNorm/gamma/adam_v with shape [768]\n",
      "Loading TF weight bert/encoder/layer_2/attention/output/dense/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_2/attention/output/dense/bias/adam_m with shape [768]\n",
      "Loading TF weight bert/encoder/layer_2/attention/output/dense/bias/adam_v with shape [768]\n",
      "Loading TF weight bert/encoder/layer_2/attention/output/dense/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_2/attention/output/dense/kernel/adam_m with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_2/attention/output/dense/kernel/adam_v with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_2/attention/self/key/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_2/attention/self/key/bias/adam_m with shape [768]\n",
      "Loading TF weight bert/encoder/layer_2/attention/self/key/bias/adam_v with shape [768]\n",
      "Loading TF weight bert/encoder/layer_2/attention/self/key/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_2/attention/self/key/kernel/adam_m with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_2/attention/self/key/kernel/adam_v with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_2/attention/self/query/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_2/attention/self/query/bias/adam_m with shape [768]\n",
      "Loading TF weight bert/encoder/layer_2/attention/self/query/bias/adam_v with shape [768]\n",
      "Loading TF weight bert/encoder/layer_2/attention/self/query/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_2/attention/self/query/kernel/adam_m with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_2/attention/self/query/kernel/adam_v with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_2/attention/self/value/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_2/attention/self/value/bias/adam_m with shape [768]\n",
      "Loading TF weight bert/encoder/layer_2/attention/self/value/bias/adam_v with shape [768]\n",
      "Loading TF weight bert/encoder/layer_2/attention/self/value/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_2/attention/self/value/kernel/adam_m with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_2/attention/self/value/kernel/adam_v with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_2/intermediate/dense/bias with shape [3072]\n",
      "Loading TF weight bert/encoder/layer_2/intermediate/dense/bias/adam_m with shape [3072]\n",
      "Loading TF weight bert/encoder/layer_2/intermediate/dense/bias/adam_v with shape [3072]\n",
      "Loading TF weight bert/encoder/layer_2/intermediate/dense/kernel with shape [768, 3072]\n",
      "Loading TF weight bert/encoder/layer_2/intermediate/dense/kernel/adam_m with shape [768, 3072]\n",
      "Loading TF weight bert/encoder/layer_2/intermediate/dense/kernel/adam_v with shape [768, 3072]\n",
      "Loading TF weight bert/encoder/layer_2/output/LayerNorm/beta with shape [768]\n",
      "Loading TF weight bert/encoder/layer_2/output/LayerNorm/beta/adam_m with shape [768]\n",
      "Loading TF weight bert/encoder/layer_2/output/LayerNorm/beta/adam_v with shape [768]\n",
      "Loading TF weight bert/encoder/layer_2/output/LayerNorm/gamma with shape [768]\n",
      "Loading TF weight bert/encoder/layer_2/output/LayerNorm/gamma/adam_m with shape [768]\n",
      "Loading TF weight bert/encoder/layer_2/output/LayerNorm/gamma/adam_v with shape [768]\n",
      "Loading TF weight bert/encoder/layer_2/output/dense/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_2/output/dense/bias/adam_m with shape [768]\n",
      "Loading TF weight bert/encoder/layer_2/output/dense/bias/adam_v with shape [768]\n",
      "Loading TF weight bert/encoder/layer_2/output/dense/kernel with shape [3072, 768]\n",
      "Loading TF weight bert/encoder/layer_2/output/dense/kernel/adam_m with shape [3072, 768]\n",
      "Loading TF weight bert/encoder/layer_2/output/dense/kernel/adam_v with shape [3072, 768]\n",
      "Loading TF weight bert/encoder/layer_3/attention/output/LayerNorm/beta with shape [768]\n",
      "Loading TF weight bert/encoder/layer_3/attention/output/LayerNorm/beta/adam_m with shape [768]\n",
      "Loading TF weight bert/encoder/layer_3/attention/output/LayerNorm/beta/adam_v with shape [768]\n",
      "Loading TF weight bert/encoder/layer_3/attention/output/LayerNorm/gamma with shape [768]\n",
      "Loading TF weight bert/encoder/layer_3/attention/output/LayerNorm/gamma/adam_m with shape [768]\n",
      "Loading TF weight bert/encoder/layer_3/attention/output/LayerNorm/gamma/adam_v with shape [768]\n",
      "Loading TF weight bert/encoder/layer_3/attention/output/dense/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_3/attention/output/dense/bias/adam_m with shape [768]\n",
      "Loading TF weight bert/encoder/layer_3/attention/output/dense/bias/adam_v with shape [768]\n",
      "Loading TF weight bert/encoder/layer_3/attention/output/dense/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_3/attention/output/dense/kernel/adam_m with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_3/attention/output/dense/kernel/adam_v with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_3/attention/self/key/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_3/attention/self/key/bias/adam_m with shape [768]\n",
      "Loading TF weight bert/encoder/layer_3/attention/self/key/bias/adam_v with shape [768]\n",
      "Loading TF weight bert/encoder/layer_3/attention/self/key/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_3/attention/self/key/kernel/adam_m with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_3/attention/self/key/kernel/adam_v with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_3/attention/self/query/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_3/attention/self/query/bias/adam_m with shape [768]\n",
      "Loading TF weight bert/encoder/layer_3/attention/self/query/bias/adam_v with shape [768]\n",
      "Loading TF weight bert/encoder/layer_3/attention/self/query/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_3/attention/self/query/kernel/adam_m with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_3/attention/self/query/kernel/adam_v with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_3/attention/self/value/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_3/attention/self/value/bias/adam_m with shape [768]\n",
      "Loading TF weight bert/encoder/layer_3/attention/self/value/bias/adam_v with shape [768]\n",
      "Loading TF weight bert/encoder/layer_3/attention/self/value/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_3/attention/self/value/kernel/adam_m with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_3/attention/self/value/kernel/adam_v with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_3/intermediate/dense/bias with shape [3072]\n",
      "Loading TF weight bert/encoder/layer_3/intermediate/dense/bias/adam_m with shape [3072]\n",
      "Loading TF weight bert/encoder/layer_3/intermediate/dense/bias/adam_v with shape [3072]\n",
      "Loading TF weight bert/encoder/layer_3/intermediate/dense/kernel with shape [768, 3072]\n",
      "Loading TF weight bert/encoder/layer_3/intermediate/dense/kernel/adam_m with shape [768, 3072]\n",
      "Loading TF weight bert/encoder/layer_3/intermediate/dense/kernel/adam_v with shape [768, 3072]\n",
      "Loading TF weight bert/encoder/layer_3/output/LayerNorm/beta with shape [768]\n",
      "Loading TF weight bert/encoder/layer_3/output/LayerNorm/beta/adam_m with shape [768]\n",
      "Loading TF weight bert/encoder/layer_3/output/LayerNorm/beta/adam_v with shape [768]\n",
      "Loading TF weight bert/encoder/layer_3/output/LayerNorm/gamma with shape [768]\n",
      "Loading TF weight bert/encoder/layer_3/output/LayerNorm/gamma/adam_m with shape [768]\n",
      "Loading TF weight bert/encoder/layer_3/output/LayerNorm/gamma/adam_v with shape [768]\n",
      "Loading TF weight bert/encoder/layer_3/output/dense/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_3/output/dense/bias/adam_m with shape [768]\n",
      "Loading TF weight bert/encoder/layer_3/output/dense/bias/adam_v with shape [768]\n",
      "Loading TF weight bert/encoder/layer_3/output/dense/kernel with shape [3072, 768]\n",
      "Loading TF weight bert/encoder/layer_3/output/dense/kernel/adam_m with shape [3072, 768]\n",
      "Loading TF weight bert/encoder/layer_3/output/dense/kernel/adam_v with shape [3072, 768]\n",
      "Loading TF weight bert/encoder/layer_4/attention/output/LayerNorm/beta with shape [768]\n",
      "Loading TF weight bert/encoder/layer_4/attention/output/LayerNorm/beta/adam_m with shape [768]\n",
      "Loading TF weight bert/encoder/layer_4/attention/output/LayerNorm/beta/adam_v with shape [768]\n",
      "Loading TF weight bert/encoder/layer_4/attention/output/LayerNorm/gamma with shape [768]\n",
      "Loading TF weight bert/encoder/layer_4/attention/output/LayerNorm/gamma/adam_m with shape [768]\n",
      "Loading TF weight bert/encoder/layer_4/attention/output/LayerNorm/gamma/adam_v with shape [768]\n",
      "Loading TF weight bert/encoder/layer_4/attention/output/dense/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_4/attention/output/dense/bias/adam_m with shape [768]\n",
      "Loading TF weight bert/encoder/layer_4/attention/output/dense/bias/adam_v with shape [768]\n",
      "Loading TF weight bert/encoder/layer_4/attention/output/dense/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_4/attention/output/dense/kernel/adam_m with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_4/attention/output/dense/kernel/adam_v with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_4/attention/self/key/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_4/attention/self/key/bias/adam_m with shape [768]\n",
      "Loading TF weight bert/encoder/layer_4/attention/self/key/bias/adam_v with shape [768]\n",
      "Loading TF weight bert/encoder/layer_4/attention/self/key/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_4/attention/self/key/kernel/adam_m with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_4/attention/self/key/kernel/adam_v with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_4/attention/self/query/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_4/attention/self/query/bias/adam_m with shape [768]\n",
      "Loading TF weight bert/encoder/layer_4/attention/self/query/bias/adam_v with shape [768]\n",
      "Loading TF weight bert/encoder/layer_4/attention/self/query/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_4/attention/self/query/kernel/adam_m with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_4/attention/self/query/kernel/adam_v with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_4/attention/self/value/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_4/attention/self/value/bias/adam_m with shape [768]\n",
      "Loading TF weight bert/encoder/layer_4/attention/self/value/bias/adam_v with shape [768]\n",
      "Loading TF weight bert/encoder/layer_4/attention/self/value/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_4/attention/self/value/kernel/adam_m with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_4/attention/self/value/kernel/adam_v with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_4/intermediate/dense/bias with shape [3072]\n",
      "Loading TF weight bert/encoder/layer_4/intermediate/dense/bias/adam_m with shape [3072]\n",
      "Loading TF weight bert/encoder/layer_4/intermediate/dense/bias/adam_v with shape [3072]\n",
      "Loading TF weight bert/encoder/layer_4/intermediate/dense/kernel with shape [768, 3072]\n",
      "Loading TF weight bert/encoder/layer_4/intermediate/dense/kernel/adam_m with shape [768, 3072]\n",
      "Loading TF weight bert/encoder/layer_4/intermediate/dense/kernel/adam_v with shape [768, 3072]\n",
      "Loading TF weight bert/encoder/layer_4/output/LayerNorm/beta with shape [768]\n",
      "Loading TF weight bert/encoder/layer_4/output/LayerNorm/beta/adam_m with shape [768]\n",
      "Loading TF weight bert/encoder/layer_4/output/LayerNorm/beta/adam_v with shape [768]\n",
      "Loading TF weight bert/encoder/layer_4/output/LayerNorm/gamma with shape [768]\n",
      "Loading TF weight bert/encoder/layer_4/output/LayerNorm/gamma/adam_m with shape [768]\n",
      "Loading TF weight bert/encoder/layer_4/output/LayerNorm/gamma/adam_v with shape [768]\n",
      "Loading TF weight bert/encoder/layer_4/output/dense/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_4/output/dense/bias/adam_m with shape [768]\n",
      "Loading TF weight bert/encoder/layer_4/output/dense/bias/adam_v with shape [768]\n",
      "Loading TF weight bert/encoder/layer_4/output/dense/kernel with shape [3072, 768]\n",
      "Loading TF weight bert/encoder/layer_4/output/dense/kernel/adam_m with shape [3072, 768]\n",
      "Loading TF weight bert/encoder/layer_4/output/dense/kernel/adam_v with shape [3072, 768]\n",
      "Loading TF weight bert/encoder/layer_5/attention/output/LayerNorm/beta with shape [768]\n",
      "Loading TF weight bert/encoder/layer_5/attention/output/LayerNorm/beta/adam_m with shape [768]\n",
      "Loading TF weight bert/encoder/layer_5/attention/output/LayerNorm/beta/adam_v with shape [768]\n",
      "Loading TF weight bert/encoder/layer_5/attention/output/LayerNorm/gamma with shape [768]\n",
      "Loading TF weight bert/encoder/layer_5/attention/output/LayerNorm/gamma/adam_m with shape [768]\n",
      "Loading TF weight bert/encoder/layer_5/attention/output/LayerNorm/gamma/adam_v with shape [768]\n",
      "Loading TF weight bert/encoder/layer_5/attention/output/dense/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_5/attention/output/dense/bias/adam_m with shape [768]\n",
      "Loading TF weight bert/encoder/layer_5/attention/output/dense/bias/adam_v with shape [768]\n",
      "Loading TF weight bert/encoder/layer_5/attention/output/dense/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_5/attention/output/dense/kernel/adam_m with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_5/attention/output/dense/kernel/adam_v with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_5/attention/self/key/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_5/attention/self/key/bias/adam_m with shape [768]\n",
      "Loading TF weight bert/encoder/layer_5/attention/self/key/bias/adam_v with shape [768]\n",
      "Loading TF weight bert/encoder/layer_5/attention/self/key/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_5/attention/self/key/kernel/adam_m with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_5/attention/self/key/kernel/adam_v with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_5/attention/self/query/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_5/attention/self/query/bias/adam_m with shape [768]\n",
      "Loading TF weight bert/encoder/layer_5/attention/self/query/bias/adam_v with shape [768]\n",
      "Loading TF weight bert/encoder/layer_5/attention/self/query/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_5/attention/self/query/kernel/adam_m with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_5/attention/self/query/kernel/adam_v with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_5/attention/self/value/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_5/attention/self/value/bias/adam_m with shape [768]\n",
      "Loading TF weight bert/encoder/layer_5/attention/self/value/bias/adam_v with shape [768]\n",
      "Loading TF weight bert/encoder/layer_5/attention/self/value/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_5/attention/self/value/kernel/adam_m with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_5/attention/self/value/kernel/adam_v with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_5/intermediate/dense/bias with shape [3072]\n",
      "Loading TF weight bert/encoder/layer_5/intermediate/dense/bias/adam_m with shape [3072]\n",
      "Loading TF weight bert/encoder/layer_5/intermediate/dense/bias/adam_v with shape [3072]\n",
      "Loading TF weight bert/encoder/layer_5/intermediate/dense/kernel with shape [768, 3072]\n",
      "Loading TF weight bert/encoder/layer_5/intermediate/dense/kernel/adam_m with shape [768, 3072]\n",
      "Loading TF weight bert/encoder/layer_5/intermediate/dense/kernel/adam_v with shape [768, 3072]\n",
      "Loading TF weight bert/encoder/layer_5/output/LayerNorm/beta with shape [768]\n",
      "Loading TF weight bert/encoder/layer_5/output/LayerNorm/beta/adam_m with shape [768]\n",
      "Loading TF weight bert/encoder/layer_5/output/LayerNorm/beta/adam_v with shape [768]\n",
      "Loading TF weight bert/encoder/layer_5/output/LayerNorm/gamma with shape [768]\n",
      "Loading TF weight bert/encoder/layer_5/output/LayerNorm/gamma/adam_m with shape [768]\n",
      "Loading TF weight bert/encoder/layer_5/output/LayerNorm/gamma/adam_v with shape [768]\n",
      "Loading TF weight bert/encoder/layer_5/output/dense/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_5/output/dense/bias/adam_m with shape [768]\n",
      "Loading TF weight bert/encoder/layer_5/output/dense/bias/adam_v with shape [768]\n",
      "Loading TF weight bert/encoder/layer_5/output/dense/kernel with shape [3072, 768]\n",
      "Loading TF weight bert/encoder/layer_5/output/dense/kernel/adam_m with shape [3072, 768]\n",
      "Loading TF weight bert/encoder/layer_5/output/dense/kernel/adam_v with shape [3072, 768]\n",
      "Loading TF weight bert/encoder/layer_6/attention/output/LayerNorm/beta with shape [768]\n",
      "Loading TF weight bert/encoder/layer_6/attention/output/LayerNorm/beta/adam_m with shape [768]\n",
      "Loading TF weight bert/encoder/layer_6/attention/output/LayerNorm/beta/adam_v with shape [768]\n",
      "Loading TF weight bert/encoder/layer_6/attention/output/LayerNorm/gamma with shape [768]\n",
      "Loading TF weight bert/encoder/layer_6/attention/output/LayerNorm/gamma/adam_m with shape [768]\n",
      "Loading TF weight bert/encoder/layer_6/attention/output/LayerNorm/gamma/adam_v with shape [768]\n",
      "Loading TF weight bert/encoder/layer_6/attention/output/dense/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_6/attention/output/dense/bias/adam_m with shape [768]\n",
      "Loading TF weight bert/encoder/layer_6/attention/output/dense/bias/adam_v with shape [768]\n",
      "Loading TF weight bert/encoder/layer_6/attention/output/dense/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_6/attention/output/dense/kernel/adam_m with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_6/attention/output/dense/kernel/adam_v with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_6/attention/self/key/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_6/attention/self/key/bias/adam_m with shape [768]\n",
      "Loading TF weight bert/encoder/layer_6/attention/self/key/bias/adam_v with shape [768]\n",
      "Loading TF weight bert/encoder/layer_6/attention/self/key/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_6/attention/self/key/kernel/adam_m with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_6/attention/self/key/kernel/adam_v with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_6/attention/self/query/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_6/attention/self/query/bias/adam_m with shape [768]\n",
      "Loading TF weight bert/encoder/layer_6/attention/self/query/bias/adam_v with shape [768]\n",
      "Loading TF weight bert/encoder/layer_6/attention/self/query/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_6/attention/self/query/kernel/adam_m with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_6/attention/self/query/kernel/adam_v with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_6/attention/self/value/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_6/attention/self/value/bias/adam_m with shape [768]\n",
      "Loading TF weight bert/encoder/layer_6/attention/self/value/bias/adam_v with shape [768]\n",
      "Loading TF weight bert/encoder/layer_6/attention/self/value/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_6/attention/self/value/kernel/adam_m with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_6/attention/self/value/kernel/adam_v with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_6/intermediate/dense/bias with shape [3072]\n",
      "Loading TF weight bert/encoder/layer_6/intermediate/dense/bias/adam_m with shape [3072]\n",
      "Loading TF weight bert/encoder/layer_6/intermediate/dense/bias/adam_v with shape [3072]\n",
      "Loading TF weight bert/encoder/layer_6/intermediate/dense/kernel with shape [768, 3072]\n",
      "Loading TF weight bert/encoder/layer_6/intermediate/dense/kernel/adam_m with shape [768, 3072]\n",
      "Loading TF weight bert/encoder/layer_6/intermediate/dense/kernel/adam_v with shape [768, 3072]\n",
      "Loading TF weight bert/encoder/layer_6/output/LayerNorm/beta with shape [768]\n",
      "Loading TF weight bert/encoder/layer_6/output/LayerNorm/beta/adam_m with shape [768]\n",
      "Loading TF weight bert/encoder/layer_6/output/LayerNorm/beta/adam_v with shape [768]\n",
      "Loading TF weight bert/encoder/layer_6/output/LayerNorm/gamma with shape [768]\n",
      "Loading TF weight bert/encoder/layer_6/output/LayerNorm/gamma/adam_m with shape [768]\n",
      "Loading TF weight bert/encoder/layer_6/output/LayerNorm/gamma/adam_v with shape [768]\n",
      "Loading TF weight bert/encoder/layer_6/output/dense/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_6/output/dense/bias/adam_m with shape [768]\n",
      "Loading TF weight bert/encoder/layer_6/output/dense/bias/adam_v with shape [768]\n",
      "Loading TF weight bert/encoder/layer_6/output/dense/kernel with shape [3072, 768]\n",
      "Loading TF weight bert/encoder/layer_6/output/dense/kernel/adam_m with shape [3072, 768]\n",
      "Loading TF weight bert/encoder/layer_6/output/dense/kernel/adam_v with shape [3072, 768]\n",
      "Loading TF weight bert/encoder/layer_7/attention/output/LayerNorm/beta with shape [768]\n",
      "Loading TF weight bert/encoder/layer_7/attention/output/LayerNorm/beta/adam_m with shape [768]\n",
      "Loading TF weight bert/encoder/layer_7/attention/output/LayerNorm/beta/adam_v with shape [768]\n",
      "Loading TF weight bert/encoder/layer_7/attention/output/LayerNorm/gamma with shape [768]\n",
      "Loading TF weight bert/encoder/layer_7/attention/output/LayerNorm/gamma/adam_m with shape [768]\n",
      "Loading TF weight bert/encoder/layer_7/attention/output/LayerNorm/gamma/adam_v with shape [768]\n",
      "Loading TF weight bert/encoder/layer_7/attention/output/dense/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_7/attention/output/dense/bias/adam_m with shape [768]\n",
      "Loading TF weight bert/encoder/layer_7/attention/output/dense/bias/adam_v with shape [768]\n",
      "Loading TF weight bert/encoder/layer_7/attention/output/dense/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_7/attention/output/dense/kernel/adam_m with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_7/attention/output/dense/kernel/adam_v with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_7/attention/self/key/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_7/attention/self/key/bias/adam_m with shape [768]\n",
      "Loading TF weight bert/encoder/layer_7/attention/self/key/bias/adam_v with shape [768]\n",
      "Loading TF weight bert/encoder/layer_7/attention/self/key/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_7/attention/self/key/kernel/adam_m with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_7/attention/self/key/kernel/adam_v with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_7/attention/self/query/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_7/attention/self/query/bias/adam_m with shape [768]\n",
      "Loading TF weight bert/encoder/layer_7/attention/self/query/bias/adam_v with shape [768]\n",
      "Loading TF weight bert/encoder/layer_7/attention/self/query/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_7/attention/self/query/kernel/adam_m with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_7/attention/self/query/kernel/adam_v with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_7/attention/self/value/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_7/attention/self/value/bias/adam_m with shape [768]\n",
      "Loading TF weight bert/encoder/layer_7/attention/self/value/bias/adam_v with shape [768]\n",
      "Loading TF weight bert/encoder/layer_7/attention/self/value/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_7/attention/self/value/kernel/adam_m with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_7/attention/self/value/kernel/adam_v with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_7/intermediate/dense/bias with shape [3072]\n",
      "Loading TF weight bert/encoder/layer_7/intermediate/dense/bias/adam_m with shape [3072]\n",
      "Loading TF weight bert/encoder/layer_7/intermediate/dense/bias/adam_v with shape [3072]\n",
      "Loading TF weight bert/encoder/layer_7/intermediate/dense/kernel with shape [768, 3072]\n",
      "Loading TF weight bert/encoder/layer_7/intermediate/dense/kernel/adam_m with shape [768, 3072]\n",
      "Loading TF weight bert/encoder/layer_7/intermediate/dense/kernel/adam_v with shape [768, 3072]\n",
      "Loading TF weight bert/encoder/layer_7/output/LayerNorm/beta with shape [768]\n",
      "Loading TF weight bert/encoder/layer_7/output/LayerNorm/beta/adam_m with shape [768]\n",
      "Loading TF weight bert/encoder/layer_7/output/LayerNorm/beta/adam_v with shape [768]\n",
      "Loading TF weight bert/encoder/layer_7/output/LayerNorm/gamma with shape [768]\n",
      "Loading TF weight bert/encoder/layer_7/output/LayerNorm/gamma/adam_m with shape [768]\n",
      "Loading TF weight bert/encoder/layer_7/output/LayerNorm/gamma/adam_v with shape [768]\n",
      "Loading TF weight bert/encoder/layer_7/output/dense/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_7/output/dense/bias/adam_m with shape [768]\n",
      "Loading TF weight bert/encoder/layer_7/output/dense/bias/adam_v with shape [768]\n",
      "Loading TF weight bert/encoder/layer_7/output/dense/kernel with shape [3072, 768]\n",
      "Loading TF weight bert/encoder/layer_7/output/dense/kernel/adam_m with shape [3072, 768]\n",
      "Loading TF weight bert/encoder/layer_7/output/dense/kernel/adam_v with shape [3072, 768]\n",
      "Loading TF weight bert/encoder/layer_8/attention/output/LayerNorm/beta with shape [768]\n",
      "Loading TF weight bert/encoder/layer_8/attention/output/LayerNorm/beta/adam_m with shape [768]\n",
      "Loading TF weight bert/encoder/layer_8/attention/output/LayerNorm/beta/adam_v with shape [768]\n",
      "Loading TF weight bert/encoder/layer_8/attention/output/LayerNorm/gamma with shape [768]\n",
      "Loading TF weight bert/encoder/layer_8/attention/output/LayerNorm/gamma/adam_m with shape [768]\n",
      "Loading TF weight bert/encoder/layer_8/attention/output/LayerNorm/gamma/adam_v with shape [768]\n",
      "Loading TF weight bert/encoder/layer_8/attention/output/dense/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_8/attention/output/dense/bias/adam_m with shape [768]\n",
      "Loading TF weight bert/encoder/layer_8/attention/output/dense/bias/adam_v with shape [768]\n",
      "Loading TF weight bert/encoder/layer_8/attention/output/dense/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_8/attention/output/dense/kernel/adam_m with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_8/attention/output/dense/kernel/adam_v with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_8/attention/self/key/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_8/attention/self/key/bias/adam_m with shape [768]\n",
      "Loading TF weight bert/encoder/layer_8/attention/self/key/bias/adam_v with shape [768]\n",
      "Loading TF weight bert/encoder/layer_8/attention/self/key/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_8/attention/self/key/kernel/adam_m with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_8/attention/self/key/kernel/adam_v with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_8/attention/self/query/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_8/attention/self/query/bias/adam_m with shape [768]\n",
      "Loading TF weight bert/encoder/layer_8/attention/self/query/bias/adam_v with shape [768]\n",
      "Loading TF weight bert/encoder/layer_8/attention/self/query/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_8/attention/self/query/kernel/adam_m with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_8/attention/self/query/kernel/adam_v with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_8/attention/self/value/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_8/attention/self/value/bias/adam_m with shape [768]\n",
      "Loading TF weight bert/encoder/layer_8/attention/self/value/bias/adam_v with shape [768]\n",
      "Loading TF weight bert/encoder/layer_8/attention/self/value/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_8/attention/self/value/kernel/adam_m with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_8/attention/self/value/kernel/adam_v with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_8/intermediate/dense/bias with shape [3072]\n",
      "Loading TF weight bert/encoder/layer_8/intermediate/dense/bias/adam_m with shape [3072]\n",
      "Loading TF weight bert/encoder/layer_8/intermediate/dense/bias/adam_v with shape [3072]\n",
      "Loading TF weight bert/encoder/layer_8/intermediate/dense/kernel with shape [768, 3072]\n",
      "Loading TF weight bert/encoder/layer_8/intermediate/dense/kernel/adam_m with shape [768, 3072]\n",
      "Loading TF weight bert/encoder/layer_8/intermediate/dense/kernel/adam_v with shape [768, 3072]\n",
      "Loading TF weight bert/encoder/layer_8/output/LayerNorm/beta with shape [768]\n",
      "Loading TF weight bert/encoder/layer_8/output/LayerNorm/beta/adam_m with shape [768]\n",
      "Loading TF weight bert/encoder/layer_8/output/LayerNorm/beta/adam_v with shape [768]\n",
      "Loading TF weight bert/encoder/layer_8/output/LayerNorm/gamma with shape [768]\n",
      "Loading TF weight bert/encoder/layer_8/output/LayerNorm/gamma/adam_m with shape [768]\n",
      "Loading TF weight bert/encoder/layer_8/output/LayerNorm/gamma/adam_v with shape [768]\n",
      "Loading TF weight bert/encoder/layer_8/output/dense/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_8/output/dense/bias/adam_m with shape [768]\n",
      "Loading TF weight bert/encoder/layer_8/output/dense/bias/adam_v with shape [768]\n",
      "Loading TF weight bert/encoder/layer_8/output/dense/kernel with shape [3072, 768]\n",
      "Loading TF weight bert/encoder/layer_8/output/dense/kernel/adam_m with shape [3072, 768]\n",
      "Loading TF weight bert/encoder/layer_8/output/dense/kernel/adam_v with shape [3072, 768]\n",
      "Loading TF weight bert/encoder/layer_9/attention/output/LayerNorm/beta with shape [768]\n",
      "Loading TF weight bert/encoder/layer_9/attention/output/LayerNorm/beta/adam_m with shape [768]\n",
      "Loading TF weight bert/encoder/layer_9/attention/output/LayerNorm/beta/adam_v with shape [768]\n",
      "Loading TF weight bert/encoder/layer_9/attention/output/LayerNorm/gamma with shape [768]\n",
      "Loading TF weight bert/encoder/layer_9/attention/output/LayerNorm/gamma/adam_m with shape [768]\n",
      "Loading TF weight bert/encoder/layer_9/attention/output/LayerNorm/gamma/adam_v with shape [768]\n",
      "Loading TF weight bert/encoder/layer_9/attention/output/dense/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_9/attention/output/dense/bias/adam_m with shape [768]\n",
      "Loading TF weight bert/encoder/layer_9/attention/output/dense/bias/adam_v with shape [768]\n",
      "Loading TF weight bert/encoder/layer_9/attention/output/dense/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_9/attention/output/dense/kernel/adam_m with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_9/attention/output/dense/kernel/adam_v with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_9/attention/self/key/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_9/attention/self/key/bias/adam_m with shape [768]\n",
      "Loading TF weight bert/encoder/layer_9/attention/self/key/bias/adam_v with shape [768]\n",
      "Loading TF weight bert/encoder/layer_9/attention/self/key/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_9/attention/self/key/kernel/adam_m with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_9/attention/self/key/kernel/adam_v with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_9/attention/self/query/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_9/attention/self/query/bias/adam_m with shape [768]\n",
      "Loading TF weight bert/encoder/layer_9/attention/self/query/bias/adam_v with shape [768]\n",
      "Loading TF weight bert/encoder/layer_9/attention/self/query/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_9/attention/self/query/kernel/adam_m with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_9/attention/self/query/kernel/adam_v with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_9/attention/self/value/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_9/attention/self/value/bias/adam_m with shape [768]\n",
      "Loading TF weight bert/encoder/layer_9/attention/self/value/bias/adam_v with shape [768]\n",
      "Loading TF weight bert/encoder/layer_9/attention/self/value/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_9/attention/self/value/kernel/adam_m with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_9/attention/self/value/kernel/adam_v with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_9/intermediate/dense/bias with shape [3072]\n",
      "Loading TF weight bert/encoder/layer_9/intermediate/dense/bias/adam_m with shape [3072]\n",
      "Loading TF weight bert/encoder/layer_9/intermediate/dense/bias/adam_v with shape [3072]\n",
      "Loading TF weight bert/encoder/layer_9/intermediate/dense/kernel with shape [768, 3072]\n",
      "Loading TF weight bert/encoder/layer_9/intermediate/dense/kernel/adam_m with shape [768, 3072]\n",
      "Loading TF weight bert/encoder/layer_9/intermediate/dense/kernel/adam_v with shape [768, 3072]\n",
      "Loading TF weight bert/encoder/layer_9/output/LayerNorm/beta with shape [768]\n",
      "Loading TF weight bert/encoder/layer_9/output/LayerNorm/beta/adam_m with shape [768]\n",
      "Loading TF weight bert/encoder/layer_9/output/LayerNorm/beta/adam_v with shape [768]\n",
      "Loading TF weight bert/encoder/layer_9/output/LayerNorm/gamma with shape [768]\n",
      "Loading TF weight bert/encoder/layer_9/output/LayerNorm/gamma/adam_m with shape [768]\n",
      "Loading TF weight bert/encoder/layer_9/output/LayerNorm/gamma/adam_v with shape [768]\n",
      "Loading TF weight bert/encoder/layer_9/output/dense/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_9/output/dense/bias/adam_m with shape [768]\n",
      "Loading TF weight bert/encoder/layer_9/output/dense/bias/adam_v with shape [768]\n",
      "Loading TF weight bert/encoder/layer_9/output/dense/kernel with shape [3072, 768]\n",
      "Loading TF weight bert/encoder/layer_9/output/dense/kernel/adam_m with shape [3072, 768]\n",
      "Loading TF weight bert/encoder/layer_9/output/dense/kernel/adam_v with shape [3072, 768]\n",
      "Loading TF weight bert/pooler/dense/bias with shape [768]\n",
      "Loading TF weight bert/pooler/dense/bias/adam_m with shape [768]\n",
      "Loading TF weight bert/pooler/dense/bias/adam_v with shape [768]\n",
      "Loading TF weight bert/pooler/dense/kernel with shape [768, 768]\n",
      "Loading TF weight bert/pooler/dense/kernel/adam_m with shape [768, 768]\n",
      "Loading TF weight bert/pooler/dense/kernel/adam_v with shape [768, 768]\n",
      "Loading TF weight cls/predictions/output_bias with shape [32000]\n",
      "Loading TF weight cls/predictions/output_bias/adam_m with shape [32000]\n",
      "Loading TF weight cls/predictions/output_bias/adam_v with shape [32000]\n",
      "Loading TF weight cls/predictions/transform/LayerNorm/beta with shape [768]\n",
      "Loading TF weight cls/predictions/transform/LayerNorm/beta/adam_m with shape [768]\n",
      "Loading TF weight cls/predictions/transform/LayerNorm/beta/adam_v with shape [768]\n",
      "Loading TF weight cls/predictions/transform/LayerNorm/gamma with shape [768]\n",
      "Loading TF weight cls/predictions/transform/LayerNorm/gamma/adam_m with shape [768]\n",
      "Loading TF weight cls/predictions/transform/LayerNorm/gamma/adam_v with shape [768]\n",
      "Loading TF weight cls/predictions/transform/dense/bias with shape [768]\n",
      "Loading TF weight cls/predictions/transform/dense/bias/adam_m with shape [768]\n",
      "Loading TF weight cls/predictions/transform/dense/bias/adam_v with shape [768]\n",
      "Loading TF weight cls/predictions/transform/dense/kernel with shape [768, 768]\n",
      "Loading TF weight cls/predictions/transform/dense/kernel/adam_m with shape [768, 768]\n",
      "Loading TF weight cls/predictions/transform/dense/kernel/adam_v with shape [768, 768]\n",
      "Loading TF weight cls/seq_relationship/output_bias with shape [2]\n",
      "Loading TF weight cls/seq_relationship/output_bias/adam_m with shape [2]\n",
      "Loading TF weight cls/seq_relationship/output_bias/adam_v with shape [2]\n",
      "Loading TF weight cls/seq_relationship/output_weights with shape [2, 768]\n",
      "Loading TF weight cls/seq_relationship/output_weights/adam_m with shape [2, 768]\n",
      "Loading TF weight cls/seq_relationship/output_weights/adam_v with shape [2, 768]\n",
      "Loading TF weight global_step with shape []\n",
      "Building PyTorch model from configuration: {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "Initialize PyTorch weight ['bert', 'embeddings', 'LayerNorm', 'beta']\n",
      "Skipping bert/embeddings/LayerNorm/beta/adam_m\n",
      "Skipping bert/embeddings/LayerNorm/beta/adam_v\n",
      "Initialize PyTorch weight ['bert', 'embeddings', 'LayerNorm', 'gamma']\n",
      "Skipping bert/embeddings/LayerNorm/gamma/adam_m\n",
      "Skipping bert/embeddings/LayerNorm/gamma/adam_v\n",
      "Initialize PyTorch weight ['bert', 'embeddings', 'position_embeddings']\n",
      "Skipping bert/embeddings/position_embeddings/adam_m\n",
      "Skipping bert/embeddings/position_embeddings/adam_v\n",
      "Initialize PyTorch weight ['bert', 'embeddings', 'token_type_embeddings']\n",
      "Skipping bert/embeddings/token_type_embeddings/adam_m\n",
      "Skipping bert/embeddings/token_type_embeddings/adam_v\n",
      "Initialize PyTorch weight ['bert', 'embeddings', 'word_embeddings']\n",
      "Skipping bert/embeddings/word_embeddings/adam_m\n",
      "Skipping bert/embeddings/word_embeddings/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "Skipping bert/encoder/layer_0/attention/output/LayerNorm/beta/adam_m\n",
      "Skipping bert/encoder/layer_0/attention/output/LayerNorm/beta/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "Skipping bert/encoder/layer_0/attention/output/LayerNorm/gamma/adam_m\n",
      "Skipping bert/encoder/layer_0/attention/output/LayerNorm/gamma/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'dense', 'bias']\n",
      "Skipping bert/encoder/layer_0/attention/output/dense/bias/adam_m\n",
      "Skipping bert/encoder/layer_0/attention/output/dense/bias/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'dense', 'kernel']\n",
      "Skipping bert/encoder/layer_0/attention/output/dense/kernel/adam_m\n",
      "Skipping bert/encoder/layer_0/attention/output/dense/kernel/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'key', 'bias']\n",
      "Skipping bert/encoder/layer_0/attention/self/key/bias/adam_m\n",
      "Skipping bert/encoder/layer_0/attention/self/key/bias/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'key', 'kernel']\n",
      "Skipping bert/encoder/layer_0/attention/self/key/kernel/adam_m\n",
      "Skipping bert/encoder/layer_0/attention/self/key/kernel/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'query', 'bias']\n",
      "Skipping bert/encoder/layer_0/attention/self/query/bias/adam_m\n",
      "Skipping bert/encoder/layer_0/attention/self/query/bias/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'query', 'kernel']\n",
      "Skipping bert/encoder/layer_0/attention/self/query/kernel/adam_m\n",
      "Skipping bert/encoder/layer_0/attention/self/query/kernel/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'value', 'bias']\n",
      "Skipping bert/encoder/layer_0/attention/self/value/bias/adam_m\n",
      "Skipping bert/encoder/layer_0/attention/self/value/bias/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'value', 'kernel']\n",
      "Skipping bert/encoder/layer_0/attention/self/value/kernel/adam_m\n",
      "Skipping bert/encoder/layer_0/attention/self/value/kernel/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'intermediate', 'dense', 'bias']\n",
      "Skipping bert/encoder/layer_0/intermediate/dense/bias/adam_m\n",
      "Skipping bert/encoder/layer_0/intermediate/dense/bias/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'intermediate', 'dense', 'kernel']\n",
      "Skipping bert/encoder/layer_0/intermediate/dense/kernel/adam_m\n",
      "Skipping bert/encoder/layer_0/intermediate/dense/kernel/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'LayerNorm', 'beta']\n",
      "Skipping bert/encoder/layer_0/output/LayerNorm/beta/adam_m\n",
      "Skipping bert/encoder/layer_0/output/LayerNorm/beta/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'LayerNorm', 'gamma']\n",
      "Skipping bert/encoder/layer_0/output/LayerNorm/gamma/adam_m\n",
      "Skipping bert/encoder/layer_0/output/LayerNorm/gamma/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'dense', 'bias']\n",
      "Skipping bert/encoder/layer_0/output/dense/bias/adam_m\n",
      "Skipping bert/encoder/layer_0/output/dense/bias/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'dense', 'kernel']\n",
      "Skipping bert/encoder/layer_0/output/dense/kernel/adam_m\n",
      "Skipping bert/encoder/layer_0/output/dense/kernel/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "Skipping bert/encoder/layer_1/attention/output/LayerNorm/beta/adam_m\n",
      "Skipping bert/encoder/layer_1/attention/output/LayerNorm/beta/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "Skipping bert/encoder/layer_1/attention/output/LayerNorm/gamma/adam_m\n",
      "Skipping bert/encoder/layer_1/attention/output/LayerNorm/gamma/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'dense', 'bias']\n",
      "Skipping bert/encoder/layer_1/attention/output/dense/bias/adam_m\n",
      "Skipping bert/encoder/layer_1/attention/output/dense/bias/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'dense', 'kernel']\n",
      "Skipping bert/encoder/layer_1/attention/output/dense/kernel/adam_m\n",
      "Skipping bert/encoder/layer_1/attention/output/dense/kernel/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'key', 'bias']\n",
      "Skipping bert/encoder/layer_1/attention/self/key/bias/adam_m\n",
      "Skipping bert/encoder/layer_1/attention/self/key/bias/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'key', 'kernel']\n",
      "Skipping bert/encoder/layer_1/attention/self/key/kernel/adam_m\n",
      "Skipping bert/encoder/layer_1/attention/self/key/kernel/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'query', 'bias']\n",
      "Skipping bert/encoder/layer_1/attention/self/query/bias/adam_m\n",
      "Skipping bert/encoder/layer_1/attention/self/query/bias/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'query', 'kernel']\n",
      "Skipping bert/encoder/layer_1/attention/self/query/kernel/adam_m\n",
      "Skipping bert/encoder/layer_1/attention/self/query/kernel/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'value', 'bias']\n",
      "Skipping bert/encoder/layer_1/attention/self/value/bias/adam_m\n",
      "Skipping bert/encoder/layer_1/attention/self/value/bias/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'value', 'kernel']\n",
      "Skipping bert/encoder/layer_1/attention/self/value/kernel/adam_m\n",
      "Skipping bert/encoder/layer_1/attention/self/value/kernel/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'intermediate', 'dense', 'bias']\n",
      "Skipping bert/encoder/layer_1/intermediate/dense/bias/adam_m\n",
      "Skipping bert/encoder/layer_1/intermediate/dense/bias/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'intermediate', 'dense', 'kernel']\n",
      "Skipping bert/encoder/layer_1/intermediate/dense/kernel/adam_m\n",
      "Skipping bert/encoder/layer_1/intermediate/dense/kernel/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'LayerNorm', 'beta']\n",
      "Skipping bert/encoder/layer_1/output/LayerNorm/beta/adam_m\n",
      "Skipping bert/encoder/layer_1/output/LayerNorm/beta/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'LayerNorm', 'gamma']\n",
      "Skipping bert/encoder/layer_1/output/LayerNorm/gamma/adam_m\n",
      "Skipping bert/encoder/layer_1/output/LayerNorm/gamma/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'dense', 'bias']\n",
      "Skipping bert/encoder/layer_1/output/dense/bias/adam_m\n",
      "Skipping bert/encoder/layer_1/output/dense/bias/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'dense', 'kernel']\n",
      "Skipping bert/encoder/layer_1/output/dense/kernel/adam_m\n",
      "Skipping bert/encoder/layer_1/output/dense/kernel/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "Skipping bert/encoder/layer_10/attention/output/LayerNorm/beta/adam_m\n",
      "Skipping bert/encoder/layer_10/attention/output/LayerNorm/beta/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "Skipping bert/encoder/layer_10/attention/output/LayerNorm/gamma/adam_m\n",
      "Skipping bert/encoder/layer_10/attention/output/LayerNorm/gamma/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'dense', 'bias']\n",
      "Skipping bert/encoder/layer_10/attention/output/dense/bias/adam_m\n",
      "Skipping bert/encoder/layer_10/attention/output/dense/bias/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'dense', 'kernel']\n",
      "Skipping bert/encoder/layer_10/attention/output/dense/kernel/adam_m\n",
      "Skipping bert/encoder/layer_10/attention/output/dense/kernel/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'key', 'bias']\n",
      "Skipping bert/encoder/layer_10/attention/self/key/bias/adam_m\n",
      "Skipping bert/encoder/layer_10/attention/self/key/bias/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'key', 'kernel']\n",
      "Skipping bert/encoder/layer_10/attention/self/key/kernel/adam_m\n",
      "Skipping bert/encoder/layer_10/attention/self/key/kernel/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'query', 'bias']\n",
      "Skipping bert/encoder/layer_10/attention/self/query/bias/adam_m\n",
      "Skipping bert/encoder/layer_10/attention/self/query/bias/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'query', 'kernel']\n",
      "Skipping bert/encoder/layer_10/attention/self/query/kernel/adam_m\n",
      "Skipping bert/encoder/layer_10/attention/self/query/kernel/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'value', 'bias']\n",
      "Skipping bert/encoder/layer_10/attention/self/value/bias/adam_m\n",
      "Skipping bert/encoder/layer_10/attention/self/value/bias/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'value', 'kernel']\n",
      "Skipping bert/encoder/layer_10/attention/self/value/kernel/adam_m\n",
      "Skipping bert/encoder/layer_10/attention/self/value/kernel/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'intermediate', 'dense', 'bias']\n",
      "Skipping bert/encoder/layer_10/intermediate/dense/bias/adam_m\n",
      "Skipping bert/encoder/layer_10/intermediate/dense/bias/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'intermediate', 'dense', 'kernel']\n",
      "Skipping bert/encoder/layer_10/intermediate/dense/kernel/adam_m\n",
      "Skipping bert/encoder/layer_10/intermediate/dense/kernel/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'LayerNorm', 'beta']\n",
      "Skipping bert/encoder/layer_10/output/LayerNorm/beta/adam_m\n",
      "Skipping bert/encoder/layer_10/output/LayerNorm/beta/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'LayerNorm', 'gamma']\n",
      "Skipping bert/encoder/layer_10/output/LayerNorm/gamma/adam_m\n",
      "Skipping bert/encoder/layer_10/output/LayerNorm/gamma/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'dense', 'bias']\n",
      "Skipping bert/encoder/layer_10/output/dense/bias/adam_m\n",
      "Skipping bert/encoder/layer_10/output/dense/bias/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'dense', 'kernel']\n",
      "Skipping bert/encoder/layer_10/output/dense/kernel/adam_m\n",
      "Skipping bert/encoder/layer_10/output/dense/kernel/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "Skipping bert/encoder/layer_11/attention/output/LayerNorm/beta/adam_m\n",
      "Skipping bert/encoder/layer_11/attention/output/LayerNorm/beta/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "Skipping bert/encoder/layer_11/attention/output/LayerNorm/gamma/adam_m\n",
      "Skipping bert/encoder/layer_11/attention/output/LayerNorm/gamma/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'dense', 'bias']\n",
      "Skipping bert/encoder/layer_11/attention/output/dense/bias/adam_m\n",
      "Skipping bert/encoder/layer_11/attention/output/dense/bias/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'dense', 'kernel']\n",
      "Skipping bert/encoder/layer_11/attention/output/dense/kernel/adam_m\n",
      "Skipping bert/encoder/layer_11/attention/output/dense/kernel/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'key', 'bias']\n",
      "Skipping bert/encoder/layer_11/attention/self/key/bias/adam_m\n",
      "Skipping bert/encoder/layer_11/attention/self/key/bias/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'key', 'kernel']\n",
      "Skipping bert/encoder/layer_11/attention/self/key/kernel/adam_m\n",
      "Skipping bert/encoder/layer_11/attention/self/key/kernel/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'query', 'bias']\n",
      "Skipping bert/encoder/layer_11/attention/self/query/bias/adam_m\n",
      "Skipping bert/encoder/layer_11/attention/self/query/bias/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'query', 'kernel']\n",
      "Skipping bert/encoder/layer_11/attention/self/query/kernel/adam_m\n",
      "Skipping bert/encoder/layer_11/attention/self/query/kernel/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'value', 'bias']\n",
      "Skipping bert/encoder/layer_11/attention/self/value/bias/adam_m\n",
      "Skipping bert/encoder/layer_11/attention/self/value/bias/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'value', 'kernel']\n",
      "Skipping bert/encoder/layer_11/attention/self/value/kernel/adam_m\n",
      "Skipping bert/encoder/layer_11/attention/self/value/kernel/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'intermediate', 'dense', 'bias']\n",
      "Skipping bert/encoder/layer_11/intermediate/dense/bias/adam_m\n",
      "Skipping bert/encoder/layer_11/intermediate/dense/bias/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'intermediate', 'dense', 'kernel']\n",
      "Skipping bert/encoder/layer_11/intermediate/dense/kernel/adam_m\n",
      "Skipping bert/encoder/layer_11/intermediate/dense/kernel/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'LayerNorm', 'beta']\n",
      "Skipping bert/encoder/layer_11/output/LayerNorm/beta/adam_m\n",
      "Skipping bert/encoder/layer_11/output/LayerNorm/beta/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'LayerNorm', 'gamma']\n",
      "Skipping bert/encoder/layer_11/output/LayerNorm/gamma/adam_m\n",
      "Skipping bert/encoder/layer_11/output/LayerNorm/gamma/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'dense', 'bias']\n",
      "Skipping bert/encoder/layer_11/output/dense/bias/adam_m\n",
      "Skipping bert/encoder/layer_11/output/dense/bias/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'dense', 'kernel']\n",
      "Skipping bert/encoder/layer_11/output/dense/kernel/adam_m\n",
      "Skipping bert/encoder/layer_11/output/dense/kernel/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "Skipping bert/encoder/layer_2/attention/output/LayerNorm/beta/adam_m\n",
      "Skipping bert/encoder/layer_2/attention/output/LayerNorm/beta/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "Skipping bert/encoder/layer_2/attention/output/LayerNorm/gamma/adam_m\n",
      "Skipping bert/encoder/layer_2/attention/output/LayerNorm/gamma/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'dense', 'bias']\n",
      "Skipping bert/encoder/layer_2/attention/output/dense/bias/adam_m\n",
      "Skipping bert/encoder/layer_2/attention/output/dense/bias/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'dense', 'kernel']\n",
      "Skipping bert/encoder/layer_2/attention/output/dense/kernel/adam_m\n",
      "Skipping bert/encoder/layer_2/attention/output/dense/kernel/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'key', 'bias']\n",
      "Skipping bert/encoder/layer_2/attention/self/key/bias/adam_m\n",
      "Skipping bert/encoder/layer_2/attention/self/key/bias/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'key', 'kernel']\n",
      "Skipping bert/encoder/layer_2/attention/self/key/kernel/adam_m\n",
      "Skipping bert/encoder/layer_2/attention/self/key/kernel/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'query', 'bias']\n",
      "Skipping bert/encoder/layer_2/attention/self/query/bias/adam_m\n",
      "Skipping bert/encoder/layer_2/attention/self/query/bias/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'query', 'kernel']\n",
      "Skipping bert/encoder/layer_2/attention/self/query/kernel/adam_m\n",
      "Skipping bert/encoder/layer_2/attention/self/query/kernel/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'value', 'bias']\n",
      "Skipping bert/encoder/layer_2/attention/self/value/bias/adam_m\n",
      "Skipping bert/encoder/layer_2/attention/self/value/bias/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'value', 'kernel']\n",
      "Skipping bert/encoder/layer_2/attention/self/value/kernel/adam_m\n",
      "Skipping bert/encoder/layer_2/attention/self/value/kernel/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'intermediate', 'dense', 'bias']\n",
      "Skipping bert/encoder/layer_2/intermediate/dense/bias/adam_m\n",
      "Skipping bert/encoder/layer_2/intermediate/dense/bias/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'intermediate', 'dense', 'kernel']\n",
      "Skipping bert/encoder/layer_2/intermediate/dense/kernel/adam_m\n",
      "Skipping bert/encoder/layer_2/intermediate/dense/kernel/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'LayerNorm', 'beta']\n",
      "Skipping bert/encoder/layer_2/output/LayerNorm/beta/adam_m\n",
      "Skipping bert/encoder/layer_2/output/LayerNorm/beta/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'LayerNorm', 'gamma']\n",
      "Skipping bert/encoder/layer_2/output/LayerNorm/gamma/adam_m\n",
      "Skipping bert/encoder/layer_2/output/LayerNorm/gamma/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'dense', 'bias']\n",
      "Skipping bert/encoder/layer_2/output/dense/bias/adam_m\n",
      "Skipping bert/encoder/layer_2/output/dense/bias/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'dense', 'kernel']\n",
      "Skipping bert/encoder/layer_2/output/dense/kernel/adam_m\n",
      "Skipping bert/encoder/layer_2/output/dense/kernel/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "Skipping bert/encoder/layer_3/attention/output/LayerNorm/beta/adam_m\n",
      "Skipping bert/encoder/layer_3/attention/output/LayerNorm/beta/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "Skipping bert/encoder/layer_3/attention/output/LayerNorm/gamma/adam_m\n",
      "Skipping bert/encoder/layer_3/attention/output/LayerNorm/gamma/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'dense', 'bias']\n",
      "Skipping bert/encoder/layer_3/attention/output/dense/bias/adam_m\n",
      "Skipping bert/encoder/layer_3/attention/output/dense/bias/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'dense', 'kernel']\n",
      "Skipping bert/encoder/layer_3/attention/output/dense/kernel/adam_m\n",
      "Skipping bert/encoder/layer_3/attention/output/dense/kernel/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'key', 'bias']\n",
      "Skipping bert/encoder/layer_3/attention/self/key/bias/adam_m\n",
      "Skipping bert/encoder/layer_3/attention/self/key/bias/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'key', 'kernel']\n",
      "Skipping bert/encoder/layer_3/attention/self/key/kernel/adam_m\n",
      "Skipping bert/encoder/layer_3/attention/self/key/kernel/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'query', 'bias']\n",
      "Skipping bert/encoder/layer_3/attention/self/query/bias/adam_m\n",
      "Skipping bert/encoder/layer_3/attention/self/query/bias/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'query', 'kernel']\n",
      "Skipping bert/encoder/layer_3/attention/self/query/kernel/adam_m\n",
      "Skipping bert/encoder/layer_3/attention/self/query/kernel/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'value', 'bias']\n",
      "Skipping bert/encoder/layer_3/attention/self/value/bias/adam_m\n",
      "Skipping bert/encoder/layer_3/attention/self/value/bias/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'value', 'kernel']\n",
      "Skipping bert/encoder/layer_3/attention/self/value/kernel/adam_m\n",
      "Skipping bert/encoder/layer_3/attention/self/value/kernel/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'intermediate', 'dense', 'bias']\n",
      "Skipping bert/encoder/layer_3/intermediate/dense/bias/adam_m\n",
      "Skipping bert/encoder/layer_3/intermediate/dense/bias/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'intermediate', 'dense', 'kernel']\n",
      "Skipping bert/encoder/layer_3/intermediate/dense/kernel/adam_m\n",
      "Skipping bert/encoder/layer_3/intermediate/dense/kernel/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'LayerNorm', 'beta']\n",
      "Skipping bert/encoder/layer_3/output/LayerNorm/beta/adam_m\n",
      "Skipping bert/encoder/layer_3/output/LayerNorm/beta/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'LayerNorm', 'gamma']\n",
      "Skipping bert/encoder/layer_3/output/LayerNorm/gamma/adam_m\n",
      "Skipping bert/encoder/layer_3/output/LayerNorm/gamma/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'dense', 'bias']\n",
      "Skipping bert/encoder/layer_3/output/dense/bias/adam_m\n",
      "Skipping bert/encoder/layer_3/output/dense/bias/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'dense', 'kernel']\n",
      "Skipping bert/encoder/layer_3/output/dense/kernel/adam_m\n",
      "Skipping bert/encoder/layer_3/output/dense/kernel/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "Skipping bert/encoder/layer_4/attention/output/LayerNorm/beta/adam_m\n",
      "Skipping bert/encoder/layer_4/attention/output/LayerNorm/beta/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "Skipping bert/encoder/layer_4/attention/output/LayerNorm/gamma/adam_m\n",
      "Skipping bert/encoder/layer_4/attention/output/LayerNorm/gamma/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'dense', 'bias']\n",
      "Skipping bert/encoder/layer_4/attention/output/dense/bias/adam_m\n",
      "Skipping bert/encoder/layer_4/attention/output/dense/bias/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'dense', 'kernel']\n",
      "Skipping bert/encoder/layer_4/attention/output/dense/kernel/adam_m\n",
      "Skipping bert/encoder/layer_4/attention/output/dense/kernel/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'key', 'bias']\n",
      "Skipping bert/encoder/layer_4/attention/self/key/bias/adam_m\n",
      "Skipping bert/encoder/layer_4/attention/self/key/bias/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'key', 'kernel']\n",
      "Skipping bert/encoder/layer_4/attention/self/key/kernel/adam_m\n",
      "Skipping bert/encoder/layer_4/attention/self/key/kernel/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'query', 'bias']\n",
      "Skipping bert/encoder/layer_4/attention/self/query/bias/adam_m\n",
      "Skipping bert/encoder/layer_4/attention/self/query/bias/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'query', 'kernel']\n",
      "Skipping bert/encoder/layer_4/attention/self/query/kernel/adam_m\n",
      "Skipping bert/encoder/layer_4/attention/self/query/kernel/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'value', 'bias']\n",
      "Skipping bert/encoder/layer_4/attention/self/value/bias/adam_m\n",
      "Skipping bert/encoder/layer_4/attention/self/value/bias/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'value', 'kernel']\n",
      "Skipping bert/encoder/layer_4/attention/self/value/kernel/adam_m\n",
      "Skipping bert/encoder/layer_4/attention/self/value/kernel/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'intermediate', 'dense', 'bias']\n",
      "Skipping bert/encoder/layer_4/intermediate/dense/bias/adam_m\n",
      "Skipping bert/encoder/layer_4/intermediate/dense/bias/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'intermediate', 'dense', 'kernel']\n",
      "Skipping bert/encoder/layer_4/intermediate/dense/kernel/adam_m\n",
      "Skipping bert/encoder/layer_4/intermediate/dense/kernel/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'LayerNorm', 'beta']\n",
      "Skipping bert/encoder/layer_4/output/LayerNorm/beta/adam_m\n",
      "Skipping bert/encoder/layer_4/output/LayerNorm/beta/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'LayerNorm', 'gamma']\n",
      "Skipping bert/encoder/layer_4/output/LayerNorm/gamma/adam_m\n",
      "Skipping bert/encoder/layer_4/output/LayerNorm/gamma/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'dense', 'bias']\n",
      "Skipping bert/encoder/layer_4/output/dense/bias/adam_m\n",
      "Skipping bert/encoder/layer_4/output/dense/bias/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'dense', 'kernel']\n",
      "Skipping bert/encoder/layer_4/output/dense/kernel/adam_m\n",
      "Skipping bert/encoder/layer_4/output/dense/kernel/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "Skipping bert/encoder/layer_5/attention/output/LayerNorm/beta/adam_m\n",
      "Skipping bert/encoder/layer_5/attention/output/LayerNorm/beta/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "Skipping bert/encoder/layer_5/attention/output/LayerNorm/gamma/adam_m\n",
      "Skipping bert/encoder/layer_5/attention/output/LayerNorm/gamma/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'dense', 'bias']\n",
      "Skipping bert/encoder/layer_5/attention/output/dense/bias/adam_m\n",
      "Skipping bert/encoder/layer_5/attention/output/dense/bias/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'dense', 'kernel']\n",
      "Skipping bert/encoder/layer_5/attention/output/dense/kernel/adam_m\n",
      "Skipping bert/encoder/layer_5/attention/output/dense/kernel/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'key', 'bias']\n",
      "Skipping bert/encoder/layer_5/attention/self/key/bias/adam_m\n",
      "Skipping bert/encoder/layer_5/attention/self/key/bias/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'key', 'kernel']\n",
      "Skipping bert/encoder/layer_5/attention/self/key/kernel/adam_m\n",
      "Skipping bert/encoder/layer_5/attention/self/key/kernel/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'query', 'bias']\n",
      "Skipping bert/encoder/layer_5/attention/self/query/bias/adam_m\n",
      "Skipping bert/encoder/layer_5/attention/self/query/bias/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'query', 'kernel']\n",
      "Skipping bert/encoder/layer_5/attention/self/query/kernel/adam_m\n",
      "Skipping bert/encoder/layer_5/attention/self/query/kernel/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'value', 'bias']\n",
      "Skipping bert/encoder/layer_5/attention/self/value/bias/adam_m\n",
      "Skipping bert/encoder/layer_5/attention/self/value/bias/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'value', 'kernel']\n",
      "Skipping bert/encoder/layer_5/attention/self/value/kernel/adam_m\n",
      "Skipping bert/encoder/layer_5/attention/self/value/kernel/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'intermediate', 'dense', 'bias']\n",
      "Skipping bert/encoder/layer_5/intermediate/dense/bias/adam_m\n",
      "Skipping bert/encoder/layer_5/intermediate/dense/bias/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'intermediate', 'dense', 'kernel']\n",
      "Skipping bert/encoder/layer_5/intermediate/dense/kernel/adam_m\n",
      "Skipping bert/encoder/layer_5/intermediate/dense/kernel/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'LayerNorm', 'beta']\n",
      "Skipping bert/encoder/layer_5/output/LayerNorm/beta/adam_m\n",
      "Skipping bert/encoder/layer_5/output/LayerNorm/beta/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'LayerNorm', 'gamma']\n",
      "Skipping bert/encoder/layer_5/output/LayerNorm/gamma/adam_m\n",
      "Skipping bert/encoder/layer_5/output/LayerNorm/gamma/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'dense', 'bias']\n",
      "Skipping bert/encoder/layer_5/output/dense/bias/adam_m\n",
      "Skipping bert/encoder/layer_5/output/dense/bias/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'dense', 'kernel']\n",
      "Skipping bert/encoder/layer_5/output/dense/kernel/adam_m\n",
      "Skipping bert/encoder/layer_5/output/dense/kernel/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "Skipping bert/encoder/layer_6/attention/output/LayerNorm/beta/adam_m\n",
      "Skipping bert/encoder/layer_6/attention/output/LayerNorm/beta/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "Skipping bert/encoder/layer_6/attention/output/LayerNorm/gamma/adam_m\n",
      "Skipping bert/encoder/layer_6/attention/output/LayerNorm/gamma/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'dense', 'bias']\n",
      "Skipping bert/encoder/layer_6/attention/output/dense/bias/adam_m\n",
      "Skipping bert/encoder/layer_6/attention/output/dense/bias/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'dense', 'kernel']\n",
      "Skipping bert/encoder/layer_6/attention/output/dense/kernel/adam_m\n",
      "Skipping bert/encoder/layer_6/attention/output/dense/kernel/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'key', 'bias']\n",
      "Skipping bert/encoder/layer_6/attention/self/key/bias/adam_m\n",
      "Skipping bert/encoder/layer_6/attention/self/key/bias/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'key', 'kernel']\n",
      "Skipping bert/encoder/layer_6/attention/self/key/kernel/adam_m\n",
      "Skipping bert/encoder/layer_6/attention/self/key/kernel/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'query', 'bias']\n",
      "Skipping bert/encoder/layer_6/attention/self/query/bias/adam_m\n",
      "Skipping bert/encoder/layer_6/attention/self/query/bias/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'query', 'kernel']\n",
      "Skipping bert/encoder/layer_6/attention/self/query/kernel/adam_m\n",
      "Skipping bert/encoder/layer_6/attention/self/query/kernel/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'value', 'bias']\n",
      "Skipping bert/encoder/layer_6/attention/self/value/bias/adam_m\n",
      "Skipping bert/encoder/layer_6/attention/self/value/bias/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'value', 'kernel']\n",
      "Skipping bert/encoder/layer_6/attention/self/value/kernel/adam_m\n",
      "Skipping bert/encoder/layer_6/attention/self/value/kernel/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'intermediate', 'dense', 'bias']\n",
      "Skipping bert/encoder/layer_6/intermediate/dense/bias/adam_m\n",
      "Skipping bert/encoder/layer_6/intermediate/dense/bias/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'intermediate', 'dense', 'kernel']\n",
      "Skipping bert/encoder/layer_6/intermediate/dense/kernel/adam_m\n",
      "Skipping bert/encoder/layer_6/intermediate/dense/kernel/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'LayerNorm', 'beta']\n",
      "Skipping bert/encoder/layer_6/output/LayerNorm/beta/adam_m\n",
      "Skipping bert/encoder/layer_6/output/LayerNorm/beta/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'LayerNorm', 'gamma']\n",
      "Skipping bert/encoder/layer_6/output/LayerNorm/gamma/adam_m\n",
      "Skipping bert/encoder/layer_6/output/LayerNorm/gamma/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'dense', 'bias']\n",
      "Skipping bert/encoder/layer_6/output/dense/bias/adam_m\n",
      "Skipping bert/encoder/layer_6/output/dense/bias/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'dense', 'kernel']\n",
      "Skipping bert/encoder/layer_6/output/dense/kernel/adam_m\n",
      "Skipping bert/encoder/layer_6/output/dense/kernel/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "Skipping bert/encoder/layer_7/attention/output/LayerNorm/beta/adam_m\n",
      "Skipping bert/encoder/layer_7/attention/output/LayerNorm/beta/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "Skipping bert/encoder/layer_7/attention/output/LayerNorm/gamma/adam_m\n",
      "Skipping bert/encoder/layer_7/attention/output/LayerNorm/gamma/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'dense', 'bias']\n",
      "Skipping bert/encoder/layer_7/attention/output/dense/bias/adam_m\n",
      "Skipping bert/encoder/layer_7/attention/output/dense/bias/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'dense', 'kernel']\n",
      "Skipping bert/encoder/layer_7/attention/output/dense/kernel/adam_m\n",
      "Skipping bert/encoder/layer_7/attention/output/dense/kernel/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'key', 'bias']\n",
      "Skipping bert/encoder/layer_7/attention/self/key/bias/adam_m\n",
      "Skipping bert/encoder/layer_7/attention/self/key/bias/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'key', 'kernel']\n",
      "Skipping bert/encoder/layer_7/attention/self/key/kernel/adam_m\n",
      "Skipping bert/encoder/layer_7/attention/self/key/kernel/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'query', 'bias']\n",
      "Skipping bert/encoder/layer_7/attention/self/query/bias/adam_m\n",
      "Skipping bert/encoder/layer_7/attention/self/query/bias/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'query', 'kernel']\n",
      "Skipping bert/encoder/layer_7/attention/self/query/kernel/adam_m\n",
      "Skipping bert/encoder/layer_7/attention/self/query/kernel/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'value', 'bias']\n",
      "Skipping bert/encoder/layer_7/attention/self/value/bias/adam_m\n",
      "Skipping bert/encoder/layer_7/attention/self/value/bias/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'value', 'kernel']\n",
      "Skipping bert/encoder/layer_7/attention/self/value/kernel/adam_m\n",
      "Skipping bert/encoder/layer_7/attention/self/value/kernel/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'intermediate', 'dense', 'bias']\n",
      "Skipping bert/encoder/layer_7/intermediate/dense/bias/adam_m\n",
      "Skipping bert/encoder/layer_7/intermediate/dense/bias/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'intermediate', 'dense', 'kernel']\n",
      "Skipping bert/encoder/layer_7/intermediate/dense/kernel/adam_m\n",
      "Skipping bert/encoder/layer_7/intermediate/dense/kernel/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'LayerNorm', 'beta']\n",
      "Skipping bert/encoder/layer_7/output/LayerNorm/beta/adam_m\n",
      "Skipping bert/encoder/layer_7/output/LayerNorm/beta/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'LayerNorm', 'gamma']\n",
      "Skipping bert/encoder/layer_7/output/LayerNorm/gamma/adam_m\n",
      "Skipping bert/encoder/layer_7/output/LayerNorm/gamma/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'dense', 'bias']\n",
      "Skipping bert/encoder/layer_7/output/dense/bias/adam_m\n",
      "Skipping bert/encoder/layer_7/output/dense/bias/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'dense', 'kernel']\n",
      "Skipping bert/encoder/layer_7/output/dense/kernel/adam_m\n",
      "Skipping bert/encoder/layer_7/output/dense/kernel/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "Skipping bert/encoder/layer_8/attention/output/LayerNorm/beta/adam_m\n",
      "Skipping bert/encoder/layer_8/attention/output/LayerNorm/beta/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "Skipping bert/encoder/layer_8/attention/output/LayerNorm/gamma/adam_m\n",
      "Skipping bert/encoder/layer_8/attention/output/LayerNorm/gamma/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'dense', 'bias']\n",
      "Skipping bert/encoder/layer_8/attention/output/dense/bias/adam_m\n",
      "Skipping bert/encoder/layer_8/attention/output/dense/bias/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'dense', 'kernel']\n",
      "Skipping bert/encoder/layer_8/attention/output/dense/kernel/adam_m\n",
      "Skipping bert/encoder/layer_8/attention/output/dense/kernel/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'key', 'bias']\n",
      "Skipping bert/encoder/layer_8/attention/self/key/bias/adam_m\n",
      "Skipping bert/encoder/layer_8/attention/self/key/bias/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'key', 'kernel']\n",
      "Skipping bert/encoder/layer_8/attention/self/key/kernel/adam_m\n",
      "Skipping bert/encoder/layer_8/attention/self/key/kernel/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'query', 'bias']\n",
      "Skipping bert/encoder/layer_8/attention/self/query/bias/adam_m\n",
      "Skipping bert/encoder/layer_8/attention/self/query/bias/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'query', 'kernel']\n",
      "Skipping bert/encoder/layer_8/attention/self/query/kernel/adam_m\n",
      "Skipping bert/encoder/layer_8/attention/self/query/kernel/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'value', 'bias']\n",
      "Skipping bert/encoder/layer_8/attention/self/value/bias/adam_m\n",
      "Skipping bert/encoder/layer_8/attention/self/value/bias/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'value', 'kernel']\n",
      "Skipping bert/encoder/layer_8/attention/self/value/kernel/adam_m\n",
      "Skipping bert/encoder/layer_8/attention/self/value/kernel/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'intermediate', 'dense', 'bias']\n",
      "Skipping bert/encoder/layer_8/intermediate/dense/bias/adam_m\n",
      "Skipping bert/encoder/layer_8/intermediate/dense/bias/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'intermediate', 'dense', 'kernel']\n",
      "Skipping bert/encoder/layer_8/intermediate/dense/kernel/adam_m\n",
      "Skipping bert/encoder/layer_8/intermediate/dense/kernel/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'LayerNorm', 'beta']\n",
      "Skipping bert/encoder/layer_8/output/LayerNorm/beta/adam_m\n",
      "Skipping bert/encoder/layer_8/output/LayerNorm/beta/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'LayerNorm', 'gamma']\n",
      "Skipping bert/encoder/layer_8/output/LayerNorm/gamma/adam_m\n",
      "Skipping bert/encoder/layer_8/output/LayerNorm/gamma/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'dense', 'bias']\n",
      "Skipping bert/encoder/layer_8/output/dense/bias/adam_m\n",
      "Skipping bert/encoder/layer_8/output/dense/bias/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'dense', 'kernel']\n",
      "Skipping bert/encoder/layer_8/output/dense/kernel/adam_m\n",
      "Skipping bert/encoder/layer_8/output/dense/kernel/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "Skipping bert/encoder/layer_9/attention/output/LayerNorm/beta/adam_m\n",
      "Skipping bert/encoder/layer_9/attention/output/LayerNorm/beta/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "Skipping bert/encoder/layer_9/attention/output/LayerNorm/gamma/adam_m\n",
      "Skipping bert/encoder/layer_9/attention/output/LayerNorm/gamma/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'dense', 'bias']\n",
      "Skipping bert/encoder/layer_9/attention/output/dense/bias/adam_m\n",
      "Skipping bert/encoder/layer_9/attention/output/dense/bias/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'dense', 'kernel']\n",
      "Skipping bert/encoder/layer_9/attention/output/dense/kernel/adam_m\n",
      "Skipping bert/encoder/layer_9/attention/output/dense/kernel/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'key', 'bias']\n",
      "Skipping bert/encoder/layer_9/attention/self/key/bias/adam_m\n",
      "Skipping bert/encoder/layer_9/attention/self/key/bias/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'key', 'kernel']\n",
      "Skipping bert/encoder/layer_9/attention/self/key/kernel/adam_m\n",
      "Skipping bert/encoder/layer_9/attention/self/key/kernel/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'query', 'bias']\n",
      "Skipping bert/encoder/layer_9/attention/self/query/bias/adam_m\n",
      "Skipping bert/encoder/layer_9/attention/self/query/bias/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'query', 'kernel']\n",
      "Skipping bert/encoder/layer_9/attention/self/query/kernel/adam_m\n",
      "Skipping bert/encoder/layer_9/attention/self/query/kernel/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'value', 'bias']\n",
      "Skipping bert/encoder/layer_9/attention/self/value/bias/adam_m\n",
      "Skipping bert/encoder/layer_9/attention/self/value/bias/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'value', 'kernel']\n",
      "Skipping bert/encoder/layer_9/attention/self/value/kernel/adam_m\n",
      "Skipping bert/encoder/layer_9/attention/self/value/kernel/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'intermediate', 'dense', 'bias']\n",
      "Skipping bert/encoder/layer_9/intermediate/dense/bias/adam_m\n",
      "Skipping bert/encoder/layer_9/intermediate/dense/bias/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'intermediate', 'dense', 'kernel']\n",
      "Skipping bert/encoder/layer_9/intermediate/dense/kernel/adam_m\n",
      "Skipping bert/encoder/layer_9/intermediate/dense/kernel/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'LayerNorm', 'beta']\n",
      "Skipping bert/encoder/layer_9/output/LayerNorm/beta/adam_m\n",
      "Skipping bert/encoder/layer_9/output/LayerNorm/beta/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'LayerNorm', 'gamma']\n",
      "Skipping bert/encoder/layer_9/output/LayerNorm/gamma/adam_m\n",
      "Skipping bert/encoder/layer_9/output/LayerNorm/gamma/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'dense', 'bias']\n",
      "Skipping bert/encoder/layer_9/output/dense/bias/adam_m\n",
      "Skipping bert/encoder/layer_9/output/dense/bias/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'dense', 'kernel']\n",
      "Skipping bert/encoder/layer_9/output/dense/kernel/adam_m\n",
      "Skipping bert/encoder/layer_9/output/dense/kernel/adam_v\n",
      "Initialize PyTorch weight ['bert', 'pooler', 'dense', 'bias']\n",
      "Skipping bert/pooler/dense/bias/adam_m\n",
      "Skipping bert/pooler/dense/bias/adam_v\n",
      "Initialize PyTorch weight ['bert', 'pooler', 'dense', 'kernel']\n",
      "Skipping bert/pooler/dense/kernel/adam_m\n",
      "Skipping bert/pooler/dense/kernel/adam_v\n",
      "Initialize PyTorch weight ['cls', 'predictions', 'output_bias']\n",
      "Skipping cls/predictions/output_bias/adam_m\n",
      "Skipping cls/predictions/output_bias/adam_v\n",
      "Initialize PyTorch weight ['cls', 'predictions', 'transform', 'LayerNorm', 'beta']\n",
      "Skipping cls/predictions/transform/LayerNorm/beta/adam_m\n",
      "Skipping cls/predictions/transform/LayerNorm/beta/adam_v\n",
      "Initialize PyTorch weight ['cls', 'predictions', 'transform', 'LayerNorm', 'gamma']\n",
      "Skipping cls/predictions/transform/LayerNorm/gamma/adam_m\n",
      "Skipping cls/predictions/transform/LayerNorm/gamma/adam_v\n",
      "Initialize PyTorch weight ['cls', 'predictions', 'transform', 'dense', 'bias']\n",
      "Skipping cls/predictions/transform/dense/bias/adam_m\n",
      "Skipping cls/predictions/transform/dense/bias/adam_v\n",
      "Initialize PyTorch weight ['cls', 'predictions', 'transform', 'dense', 'kernel']\n",
      "Skipping cls/predictions/transform/dense/kernel/adam_m\n",
      "Skipping cls/predictions/transform/dense/kernel/adam_v\n",
      "Initialize PyTorch weight ['cls', 'seq_relationship', 'output_bias']\n",
      "Skipping cls/seq_relationship/output_bias/adam_m\n",
      "Skipping cls/seq_relationship/output_bias/adam_v\n",
      "Initialize PyTorch weight ['cls', 'seq_relationship', 'output_weights']\n",
      "Skipping cls/seq_relationship/output_weights/adam_m\n",
      "Skipping cls/seq_relationship/output_weights/adam_v\n",
      "Skipping global_step\n",
      "Save PyTorch model to model/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "!python3 convert_tf_checkpoint_to_pytorch.py --tf_checkpoint_path model/model.ckpt-952000 --bert_config_file model/bert_config.json --pytorch_dump_path model/pytorch_model.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "Gu609Wyruly2",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 5253.0
    },
    "outputId": "974f9261-d30c-4b3e-8a08-c97c17892074"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_pretrained_bert.modeling:loading archive file ./model/\n",
      "INFO:pytorch_pretrained_bert.modeling:Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(32000, 768)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): BertLayerNorm()\n",
       "    (dropout): Dropout(p=0.1)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load pre-trained model (weights)\n",
    "model = BertModel.from_pretrained('./model/')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "rUoSqaf3vth0",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 4998.0
    },
    "outputId": "ac9e9e76-9e3c-4d35-ad86-2c13ded78b4f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(32000, 768)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): BertLayerNorm()\n",
       "    (dropout): Dropout(p=0.1)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you have a GPU, put everything on cuda\n",
    "tokens_tensor = tokens_tensor.to('cuda')\n",
    "segments_tensors = segments_tensors.to('cuda')\n",
    "model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "qo-HfT_Avi8d",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "# Predict hidden states features for each layer\n",
    "with torch.no_grad():\n",
    "    encoded_layers, _ = model(tokens_tensor, segments_tensors)\n",
    "# We have a hidden states for each of the 12 layers in model bert-base-uncased\n",
    "assert len(encoded_layers) == 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "nuknNf8Avilm",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289.0
    },
    "outputId": "fd080609-673b-44ff-e143-87e4b8868128"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_pretrained_bert.modeling:loading archive file ./model/\n",
      "INFO:pytorch_pretrained_bert.modeling:Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "INFO:pytorch_pretrained_bert.modeling:Weights from pretrained model not used in BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained model (weights)\n",
    "model = BertForMaskedLM.from_pretrained('./model/')\n",
    "model.eval()\n",
    "\n",
    "# If you have a GPU, put everything on cuda\n",
    "tokens_tensor = tokens_tensor.to('cuda')\n",
    "segments_tensors = segments_tensors.to('cuda')\n",
    "model.to('cuda')\n",
    "\n",
    "# Predict all tokens\n",
    "with torch.no_grad():\n",
    "    predictions = model(tokens_tensor, segments_tensors)\n",
    "\n",
    "# confirm we were able to predict 'henson'\n",
    "predicted_index = torch.argmax(predictions[0, masked_index]).item()\n",
    "predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]\n",
    "assert predicted_token == '▁Чингис'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "g3AYFGLq5FgD",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34.0
    },
    "outputId": "18386c87-33a0-44ef-b47a-482b197e3d2e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'▁Чингис'"
      ]
     },
     "execution_count": 49,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "eF5cHzBQ7ETY",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "MLM check.ipynb",
   "version": "0.3.2",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
